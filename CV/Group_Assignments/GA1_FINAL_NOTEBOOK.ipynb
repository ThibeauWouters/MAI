{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Preamble: install and import packages","metadata":{}},{"cell_type":"markdown","source":"**Important notice** due to OutOfMemory errors, we use different input data. Below, we define the paths for this Kaggle notebook. We also import preprocessed test images (see discussion in notebook) as well as 2 template images.","metadata":{}},{"cell_type":"code","source":"train_path = \"/kaggle/input/traindata\"\ntest_path = \"/kaggle/input/test-preprocessed\"\ntemplate_path = \"/kaggle/input/templates\"","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:59:03.852668Z","iopub.execute_input":"2023-04-12T17:59:03.853167Z","iopub.status.idle":"2023-04-12T17:59:03.892904Z","shell.execute_reply.started":"2023-04-12T17:59:03.853121Z","shell.execute_reply":"2023-04-12T17:59:03.891824Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"%%capture\n%pip install deepface mtcnn","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:59:05.290034Z","iopub.execute_input":"2023-04-12T17:59:05.290531Z","iopub.status.idle":"2023-04-12T17:59:33.768297Z","shell.execute_reply.started":"2023-04-12T17:59:05.290464Z","shell.execute_reply":"2023-04-12T17:59:33.766967Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import io # Input/Output Module\nimport os # OS interfaces\nimport numpy as np # linear algebra\nimport cv2 # OpenCV package\nfrom matplotlib import pyplot as plt # Plotting library\nplt.rcParams[\"figure.figsize\"] = (8, 5)\nimport seaborn as sns # diffeerent plotting library\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pickle  # import pickle, to read and save variables\nimport time # measure elapsed time in execution\nfrom typing import Callable # type hinting\nfrom tqdm import tqdm # progress bar\n# Scikit learn\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.svm import SVC\nfrom sklearn.manifold import TSNE\nfrom sklearn.model_selection import RandomizedSearchCV, train_test_split, cross_val_predict\nfrom sklearn.utils.fixes import loguniform\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score, multilabel_confusion_matrix, matthews_corrcoef\nfrom sklearn.decomposition import PCA\nfrom skimage.exposure import rescale_intensity\nfrom urllib import request # module for opening HTTP requests\n# MTCNN, or Multi-Task Cascaded Convolutional Neural Networks\nfrom mtcnn.mtcnn import MTCNN  \n# DeepFace\nfrom deepface import DeepFace\nfrom deepface.detectors import FaceDetector\n# PyTorch\nimport torch.optim\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.preprocessing import StandardScaler\n# Import VGG from Keras for final pipeline\nfrom keras.applications.vgg16 import preprocess_input as vgg16_preprocess_input\nfrom keras.models import Model\nfrom keras.layers import Dense, Flatten, Input\nfrom keras.optimizers import Adam\nimport keras.callbacks","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.230891,"end_time":"2021-03-08T07:57:06.335029","exception":false,"start_time":"2021-03-08T07:57:06.104138","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-12T17:59:33.770812Z","iopub.execute_input":"2023-04-12T17:59:33.771115Z","iopub.status.idle":"2023-04-12T17:59:43.572972Z","shell.execute_reply.started":"2023-04-12T17:59:33.771079Z","shell.execute_reply":"2023-04-12T17:59:43.571886Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Directory  /root /.deepface created\nDirectory  /root /.deepface/weights created\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<div style=\"width:100%; height:140px\">\n    <img src=\"https://www.kuleuven.be/internationaal/thinktank/fotos-en-logos/ku-leuven-logo.png/image_preview\" width = 300px, heigh = auto align=left>\n</div>\n\n\n# KUL H02A5a Computer Vision: Group Assignment 1\n\nStudent numbers: <span style=\"color:red\">r0708518, r0927391, r0925509, r0924356, r0912639</span>.\n\nThe goal of this assignment is to explore more advanced techniques for constructing features that better describe objects of interest and to perform face recognition using these features. This assignment will be delivered in groups of 5 (either composed by you or randomly assigned by your TA's).\n\nIn this assignment you are a group of computer vision experts that have been invited to ECCV 2021 to do a tutorial about  \"Feature representations, then and now\". To prepare the tutorial you are asked to participate in a kaggle competition and to release a notebook that can be easily studied by the tutorial participants. Your target audience is: (master) students who want to get a first hands-on introduction to the techniques that you apply.\n\n---------------------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"# Feature representations: then and now\n\nWelcome to the workshop on feature representations and their use in face recognition! This tutorial will take you from the basics of loading images to building handcrafted feature representations of faces and eventually introduces deep learning as a way to generate features.\nThis notebook is structured as follows:\n\n0. Data loading & Preprocessing\n1. Feature Representations\n2. Evaluation Metrics \n3. Classifiers\n4. Experiments\n5. Publishing best results\n6. Discussion\n\nThe most important contributions (improvements on the template notebook) are signified with an exclamation mark in the section title.\n\n<!--\n\nMake sure that your notebook is **self-contained** and **fully documented**. Walk us through all steps of your code. Treat your notebook as a tutorial for students who need to get a first hands-on introduction to the techniques that you apply. Provide strong arguments for the design choices that you made and what insights you got from your experiments. Make use of the *Group assignment* forum/discussion board on Toledo if you have any questions.\n\nFill in your student numbers above and get to it! Good luck! \n\n\n<div class=\"alert alert-block alert-info\">\n<b>NOTE:</b> This notebook is just a example/template, feel free to adjust in any way you please! Just keep things organised and document accordingly!\n</div>\n\n<div class=\"alert alert-block alert-info\">\n<b>NOTE:</b> Clearly indicate the improvements that you make!!! You can for instance use titles like: <i>3.1. Improvement: Non-linear SVM with RBF Kernel.<i>\n</div>\n-->","metadata":{"papermill":{"duration":0.022868,"end_time":"2021-03-08T07:57:06.382109","exception":false,"start_time":"2021-03-08T07:57:06.359241","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# 0. Data loading & Preprocessing\n\nWe will start the tutorial by reading the training and testing image data provided by the Kaggle competition. Next, we will preprocess the data in order to extract faces out of the images and distill features from them.\n\n## 0.1. Loading data\nYou will notice that the training set is many times smaller than the test set. While this might strike you as odd, this is close to a real world scenario where your system might be put through daily use! In this session we will try to do the best we can with the data that we've got! *Note: this dataset is a subset of the* [*VGG face dataset*](https://www.robots.ox.ac.uk/~vgg/data/vgg_face/).","metadata":{}},{"cell_type":"code","source":"# Point to correct data\nbase_file_path = train_path\n\n# Read train data with pandas csv\ntrain = pd.read_csv(\n    os.path.join(base_file_path, 'train_set.csv'), index_col = 0)\ntrain.index = train.index.rename('id')\n\n# Read test data with pandas csv\ntest = pd.read_csv(\n    os.path.join(base_file_path, 'test_set.csv'), index_col = 0)\ntest.index = test.index.rename('id')\n\n# Read the images as numpy arrays and store in \"img\" column\ntrain['img'] = [cv2.cvtColor(np.load(os.path.join(base_file_path, 'train/train_{}.npy').format(index), allow_pickle=False), cv2.COLOR_BGR2RGB) \n                for index, row in train.iterrows()]\n\n## We processed the test image and will load it again afterwards, we ignore the raw files for memory reasons!\n# test['img'] = [cv2.cvtColor(np.load(os.path.join(base_file_path, 'test/test_{}.npy').format(index), allow_pickle=False), cv2.COLOR_BGR2RGB) \n#                 for index, row in test.iterrows()]\n\n# Report sizes of train and test data\ntrain_size, test_size = len(train), len(test)\n\"The training set contains {} examples, the test set contains {} examples.\".format(train_size, test_size)","metadata":{"papermill":{"duration":37.543619,"end_time":"2021-03-08T07:57:43.9495","exception":false,"start_time":"2021-03-08T07:57:06.405881","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-12T18:02:47.895829Z","iopub.execute_input":"2023-04-12T18:02:47.896356Z","iopub.status.idle":"2023-04-12T18:02:48.128075Z","shell.execute_reply.started":"2023-04-12T18:02:47.896311Z","shell.execute_reply":"2023-04-12T18:02:48.127050Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"'The training set contains 80 examples, the test set contains 1816 examples.'"},"metadata":{}}]},{"cell_type":"markdown","source":"**Warning:** Enabling plots is likely to crash browsers! We define a global variable which allows us to enable or disable showing the plots, as plotting all images can slow down the notebook.","metadata":{}},{"cell_type":"code","source":"# Show important plots (for discussion etc)\nSHOW_PLOTS = False\n# Show preprocessor plots (a lot of images, not so crucial)\nSHOW_PREPROCESSOR_PLOTS = False","metadata":{"execution":{"iopub.status.busy":"2023-04-12T18:02:50.888553Z","iopub.execute_input":"2023-04-12T18:02:50.889745Z","iopub.status.idle":"2023-04-12T18:02:50.894885Z","shell.execute_reply.started":"2023-04-12T18:02:50.889700Z","shell.execute_reply":"2023-04-12T18:02:50.893931Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## 0.2. A first look\nLet's have a look at the data columns and class distributions. We can see that the training set contains an identifier, name of the person, the image as an array of pixel values, and the class label:","metadata":{}},{"cell_type":"code","source":"train.head(1)","metadata":{"execution":{"iopub.execute_input":"2023-04-12T13:53:37.859119Z","iopub.status.busy":"2023-04-12T13:53:37.858412Z","iopub.status.idle":"2023-04-12T13:53:40.227211Z","shell.execute_reply":"2023-04-12T13:53:40.225869Z","shell.execute_reply.started":"2023-04-12T13:53:37.859080Z"},"papermill":{"duration":3.315629,"end_time":"2021-03-08T07:57:47.336913","exception":false,"start_time":"2021-03-08T07:57:44.021284","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The test set, on the other hand, does *not* have a class label: our goal will be to predict this using the information of the training data! (*note:* by default, we do not read the raw test images: in that case, the `img` column shows -1)","metadata":{}},{"cell_type":"code","source":"test.head(1)","metadata":{"execution":{"iopub.execute_input":"2023-04-12T13:54:14.598506Z","iopub.status.busy":"2023-04-12T13:54:14.597304Z","iopub.status.idle":"2023-04-12T13:54:14.607614Z","shell.execute_reply":"2023-04-12T13:54:14.606348Z","shell.execute_reply.started":"2023-04-12T13:54:14.598441Z"},"papermill":{"duration":3.283501,"end_time":"2021-03-08T07:57:50.644778","exception":false,"start_time":"2021-03-08T07:57:47.361277","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can also look at the distributions of the different classes inside the training set:","metadata":{}},{"cell_type":"code","source":"train.groupby('name').agg({'img':'count', 'class': 'max'})","metadata":{"execution":{"iopub.execute_input":"2023-04-12T13:54:15.732659Z","iopub.status.busy":"2023-04-12T13:54:15.731931Z","iopub.status.idle":"2023-04-12T13:54:15.747198Z","shell.execute_reply":"2023-04-12T13:54:15.746091Z","shell.execute_reply.started":"2023-04-12T13:54:15.732618Z"},"papermill":{"duration":0.046628,"end_time":"2021-03-08T07:57:50.716317","exception":false,"start_time":"2021-03-08T07:57:50.669689","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note from the above table that **Jesse is assigned the classification label 1**, and **Mila is assigned the classification label 2**. The dataset also contains 20 images of **look alikes (assigned classification label 0)** and the raw images. ","metadata":{}},{"cell_type":"markdown","source":"## 0.3. Preprocess data\n\n\n<!-- <div class=\"alert alert-block alert-info\"> <b>NOTE:</b> You can write temporary files to <code>/kaggle/temp/</code> or <code>../../tmp</code>, but they won't be saved outside of the current session\n</div> -->\n","metadata":{"papermill":{"duration":0.025108,"end_time":"2021-03-08T07:57:50.766719","exception":false,"start_time":"2021-03-08T07:57:50.741611","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Now that our data is loaded into the notebook, we will preprocess the images and detect faces present in the image.\nFirst, we declare a variable that will set the final size of the faces for our pipeline. We chose for a size of (224, 244), which is the size used in the VGG-16 architecture that gives the best results in the final pipeline. Apart from that, we also define an auxiliary function to facilitate the plotting of a sequence of images.","metadata":{}},{"cell_type":"code","source":"FACE_SIZE = (224, 224)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T18:02:56.009317Z","iopub.execute_input":"2023-04-12T18:02:56.009713Z","iopub.status.idle":"2023-04-12T18:02:56.015549Z","shell.execute_reply.started":"2023-04-12T18:02:56.009678Z","shell.execute_reply":"2023-04-12T18:02:56.013684Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def plot_image_sequence(data, imgs_per_row=10, cmap=\"Greys_r\"):\n    \"\"\"Auxiliary function that plots a sequence of images in convenient format.\n    Args:\n        data (np.array): Sequence of images\n        imgs_per_row (int, optional): Number of images on each row. Defaults to 7.\n        cmap (str, optional): Colormap used in plotting. Defaults to \"Greys_r\" for greyscale images.\n    \"\"\"\n    n = len(data)\n    n_rows = n//imgs_per_row\n    if n%imgs_per_row != 0:\n        n_rows += 1\n    n_cols = imgs_per_row\n    fig, axs = plt.subplots(n_rows,n_cols, figsize=(10*n_cols,10*n_rows))\n    for i in range(n):\n        ax = axs[i//imgs_per_row, i%imgs_per_row]\n        ax.imshow(data[i], cmap=cmap)\n    # Disable ticks\n    for row in axs:\n        for ax in row:\n            ax.set_xticks([])\n            ax.set_yticks([])\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-04-12T18:02:56.211857Z","iopub.execute_input":"2023-04-12T18:02:56.212565Z","iopub.status.idle":"2023-04-12T18:02:56.220256Z","shell.execute_reply.started":"2023-04-12T18:02:56.212521Z","shell.execute_reply":"2023-04-12T18:02:56.219196Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"We also define an auxiliary function that, given an image and a face detected inside that image (specified by a bounding box around the face), allows us to modify the bounding box and e.g. enlarge it to include features such as hairstyle which are usually cropped from images by most face detectors.","metadata":{}},{"cell_type":"code","source":"def cut_out_face(img: np.array, x: int, y: int, width: int, height: int, width_factor: float = 0.1, height_factor: float = -1, \n                 center_x: int = None, center_y: int = None, square: bool = True) -> np.array:\n    \"\"\"\n    Cuts out the face detected in an image with a modified bounding box.\n    Args:\n        img (np.array): Original image.\n        x (int): The x coordinate of the bounding box of the detected face.\n        y (int): The y coordinate of the bounding box of the detected face.\n        width (int): The width of the bounding box of the detected face.\n        height (int): The height of the bounding box of the detected face.\n        width_factor (float, optional): Multiplier increasing the width of the bounding box. Defaults to 0.1.\n        height_factor (float, optional): Multiplier increasing the height of the bounding box. Defaults to -1, such that it is set by the value of width_factor.\n        center_x (int, optional): The x coordinate of a keypoint, such as nose or mouth. Defaults to None.\n        center_y (int, optional): The y coordinate of a keypoint, such as nose or mouth. Defaults to None.\n        square (boolean, optional): Specify whether bounding box has to be square-shaped. Defaults to True.\n    Returns:\n        np.array: Face cropped out of the original image.\n    \"\"\"\n    \n    # If height factor not set, default it to twice width factor\n    if height_factor < 0:\n        height_factor = 2 * width_factor\n    \n    # If we want the images to be square, adjust height or width (go for smallest square)\n    if square:\n        if width <= height:\n            height = width\n        else:\n            width = height\n            \n    # In case a central point is provided, shift the coordinates to center that object\n    if center_x is not None and center_y is not None:\n        x = center_x - width/2\n        y = center_y - height/2\n    \n    # Get the width and height of the image\n    img_height, img_width = img.shape[0], img.shape[1]\n    \n    # Get new initial positions\n    new_x = max(0, int(x - width_factor*width))\n    new_y = max(0, int(y - height_factor*height))\n    \n    new_width  = min(img_width  - new_x, int((1 + 2*width_factor)*width))\n    new_height = min(img_height - new_y, int((1 + 2*height_factor)*height))\n    \n    return img[new_y:new_y+new_height, new_x:new_x+new_width]","metadata":{"execution":{"iopub.status.busy":"2023-04-12T18:02:57.159216Z","iopub.execute_input":"2023-04-12T18:02:57.159744Z","iopub.status.idle":"2023-04-12T18:02:57.171659Z","shell.execute_reply.started":"2023-04-12T18:02:57.159698Z","shell.execute_reply":"2023-04-12T18:02:57.170539Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Since the face detectors fail to detect a face in an image occasionally (especially less refined techniques such as the HAAR detector), we will write an auxiliary function that gets the indices (inside a dataframe) of images that contain negative pixel values, which signals a failed detection.","metadata":{}},{"cell_type":"code","source":"def detect_negatives_images(imgs: np.array, verbose: bool = False) -> list:\n    \"\"\"\n    Auxiliary function that saves the indices of images in a Numpy array that contain negative pixel values.\n    Args:\n        imgs (np.array): Sequence of images to be checked for negative pixel values.\n        verbose (bool, optional): Signal whether to print the indices containing negative pixel values. Defaults to False.\n    Returns:\n        list: A list containing the indices containing negative pixel values.\n    \"\"\"\n    # Save indices of images that have negative pixel values\n    indices_to_delete = []\n    for i, img in enumerate(imgs):\n        if np.any(img.flatten() < 0):\n            if verbose:\n                print(f\"Negative values in image {i} detected\")\n            indices_to_delete.append(i)\n    return indices_to_delete","metadata":{"execution":{"iopub.status.busy":"2023-04-12T18:02:58.071811Z","iopub.execute_input":"2023-04-12T18:02:58.072471Z","iopub.status.idle":"2023-04-12T18:02:58.079184Z","shell.execute_reply.started":"2023-04-12T18:02:58.072433Z","shell.execute_reply":"2023-04-12T18:02:58.077758Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"#### 0.3.1 Data augmentation/enhancement (!)\n\nAs already mentioned, the training data has just 80 examples, which is a quite low compared to the test set. In addition to that, some pictures have multiple people in them, are a montage of different pictures of the same person or are a meme mixing real people and cartoon figures. Therefore, in this section we will do a couple of things to improve the overall quality of the training data.\n\nWe will do a quick, manual crop of some images. Sometimes, cropping an image will make it easier for the face detectors to detect the correct face (based on the corresponding class label), since pictures that include 2 or more people will get simplified to just show the most important one (Sarah, Jesse, Michael or Mila). At the same time, some montages or memes can include different people we are interested in, that is why we will also perform some crops so that we can divide the pictures in 2 different ones with new labels that will increase the size of the data. In the table below, we summarize all augmentations/enhancement that we then perform in a cell below the table.","metadata":{}},{"cell_type":"markdown","source":"<style>\n  :root {\n    --col-width: 500px;\n  }\n  \n  table {\n    width: 100%;\n    border-collapse: collapse;\n  }\n  \n  th,\n  td {\n    padding: 8px;\n    border: 1px solid black;\n  }\n  \n  th:nth-child(1) {\n    width: var(--col-width);\n  }\n  \n  th:nth-child(2),\n  th:nth-child(3) {\n    width: calc((100% - var(--col-width)) / 2);\n    max-width: var(--col-width);\n  }\n  .my-table th:nth-child(1) {\n  width: var(--col-width);\n}\n\n.my-table th:nth-child(2),\n.my-table th:nth-child(3) {\n  width: calc((100% - var(--col-width)) / 2);\n  max-width: var(--col-width);\n}\n</style>\n<table class=\"my-table\">\n  <thead>\n    <tr>\n      <th>Index</th>\n      <th>Before</th>\n      <th>After</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>18</td>\n      <td>Picture shows both Jesse and Michael, but is classified as Jesse (label = 1).</td>\n      <td>Image at Index 18 just shows Jesse, and new image at Index 80 shows Michael with label = 0</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>The picture shows both Michael and a cartoon.</td>\n      <td>Image cropped to just show Michael.</td>\n    </tr>\n      <tr>\n      <td>28</td>\n      <td>The picture shows Mila with 2 other people.</td>\n      <td>Image cropped to just show Mila.</td>\n    </tr>\n      <tr>\n      <td>29</td>\n      <td>The picture shows Mila with another person.</td>\n      <td>Image cropped to just show Mila.</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>The picture shows two pictures of Mila.</td>\n      <td>Image separated to get two samples from it (Indexes 30 and 81).</td>\n    </tr>\n    <tr>\n    <td>32</td>\n      <td>The picture shows two pictures of Sarah.</td>\n      <td>Image separated to get two samples from it (Indexes 32 &amp; 82).</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>The picture shows Michael, Jesse and a third person in a Pokémon meme.</td>\n      <td>Image cropped to get Jesse at Index 34 and Michael at new Index 83.</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>The picture shows Mila with another person.</td>\n      <td>Image cropped to just get Mila.</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>The picture shows Mila with another person.</td>\n      <td>Image cropped to just get Mila.</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>The picture shows Michael with another person.</td>\n      <td>Image cropped to just get him.</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>The picture shows Jesse with another person.</td>\n      <td>Image cropped to just get him.</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>The picture shows Mila with another person.</td>\n      <td>Image cropped to just get her.</td>\n    </tr>\n    <tr>\n      <td>52</td>\n      <td>The picture shows Jesse with another person.</td>\n      <td>Image cropped to just get him.</td>\n    </tr>\n    <tr>\n      <td>53</td>\n      <td>The picture shows Jesse with another person.</td>\n      <td>Image cropped to just get him.</td>\n    </tr>\n    <tr>\n      <td>57</td>\n      <td>The picture shows 2 photos of Mila.</td>\n      <td>Image cropped to get one at Index 57 and the other at new Index 84.</td>\n    </tr>\n    <tr>\n      <td>59</td>\n      <td>The picture shows a photo of Sarah with another person.</td>\n      <td>Image cropped to just get her</td>\n    </tr>\n    <tr>\n      <td>61</td>\n      <td>The picture shows a photo of Mila with another person.</td>\n      <td>Image cropped to just get her</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>The picture shows a photo of Jesse with 2 other people.</td>\n      <td>Image cropped to just get him.</td>\n    </tr>\n    <tr>\n      <td>77</td>\n      <td>The picture shows 2 photos of Mila.</td>\n      <td>Image cropped to get one at Index 77 and the other at new Index 85.</td>\n    </tr>\n    <tr>\n      <td>65</td>\n      <td>The picture shows an \"image not found\" placeholder, due to possibly the data being corrupted.</td>\n      <td>Image is deleted from improved training data.</td>\n    </tr>\n  </tbody>\n</table>\n","metadata":{}},{"cell_type":"code","source":"# Copy the original training data, going to improve upon that below\ntrain_improved = train.copy()\nprint(\"Length of TRAIN df: \" + str(len(train_improved.index)))\n# Index 18\naux = pd.DataFrame({\"name\": [\"Michael_Cera\"], \"class\": [0], \"img\": [train_improved.loc[18].img[:, 270:]]})\ntrain_improved = train_improved.append(aux, ignore_index=True)\ntrain_improved[\"img\"][18] = train_improved.loc[18].img[:, :260]\n# Index 25\ntrain_improved[\"img\"][25] = train_improved.loc[25].img[:, 200:]\n# Index 28\ntrain_improved[\"img\"][28] = train_improved.loc[28].img[:, 275:625]\n# Index 29\ntrain_improved[\"img\"][29] = train_improved.loc[29].img[:, :400]\n# Index 30\naux = pd.DataFrame({\"name\": [\"Mila_Kunis\"], \"class\": [2], \"img\": [train_improved.loc[30].img[:, :100]]})\ntrain_improved = train_improved.append(aux, ignore_index=True)\ntrain_improved[\"img\"][30] = train_improved.loc[30].img[:, 100:]\n# Index 32\naux = pd.DataFrame({\"name\": [\"Sarah_Hyland\"], \"class\": [0], \"img\": [train_improved.loc[32].img[:, 420:]]})\ntrain_improved = train_improved.append(aux, ignore_index=True)\ntrain_improved[\"img\"][32] = train_improved.loc[32].img[:, :420]\n# Index 34\naux = pd.DataFrame({\"name\": [\"Michael_Cera\"], \"class\": [0], \"img\": [train_improved.loc[34].img[160:, :160]]})\ntrain_improved = train_improved.append(aux, ignore_index=True)\ntrain_improved[\"img\"][34] = train_improved.loc[34].img[160:, 210:380]\n# Index 39\ntrain_improved[\"img\"][39] = train_improved.loc[39].img[:, 125:220]\n# Index 40\ntrain_improved[\"img\"][40] = train_improved.loc[40].img[:, 107:]\n# Index 41\ntrain_improved[\"img\"][41] = train_improved.loc[41].img[:, :200]\n# Index 49\ntrain_improved[\"img\"][49] = train_improved.loc[49].img[:, :150]\n# Index 50\ntrain_improved[\"img\"][50] = train_improved.loc[50].img[:200, :]\n# Index 52\ntrain_improved[\"img\"][52] = train_improved.loc[52].img[:, 325:]\n# Index 53\ntrain_improved[\"img\"][53] = train_improved.loc[53].img[:, 300:]\n# Index 57\naux = pd.DataFrame({\"name\": [\"Mila_Kunis\"], \"class\": [2], \"img\": [train_improved.loc[57].img[:, :100]]})\ntrain_improved = train_improved.append(aux, ignore_index=True)\ntrain_improved[\"img\"][57] = train_improved.loc[57].img[:, 100:]\n# Index 59\ntrain_improved[\"img\"][59] = train_improved.loc[59].img[:, 220:]\n# index 61\ntrain_improved[\"img\"][61] = train_improved.loc[61].img[:, 200:]\n# Index 70\ntrain_improved[\"img\"][70] = train_improved.loc[70].img[:, 175:]\n# Index 77\naux = pd.DataFrame({\"name\": [\"Mila_Kunis\"], \"class\": [2], \"img\": [train_improved.loc[77].img[:, :100]]})\ntrain_improved = train_improved.append(aux, ignore_index=True)\ntrain_improved[\"img\"][77] = train_improved.loc[77].img[:, 100:]\n# Index 65\ntrain_improved = train_improved.drop([65])\nprint(\"Length of improved TRAIN df: \" + str(len(train_improved.index)))","metadata":{"execution":{"iopub.status.busy":"2023-04-12T18:03:00.251555Z","iopub.execute_input":"2023-04-12T18:03:00.252626Z","iopub.status.idle":"2023-04-12T18:03:00.297085Z","shell.execute_reply.started":"2023-04-12T18:03:00.252570Z","shell.execute_reply":"2023-04-12T18:03:00.295980Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Length of TRAIN df: 80\nLength of improved TRAIN df: 85\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 0.3.2 HAAR preprocessor","metadata":{}},{"cell_type":"markdown","source":"Now that we cleaned up and augmented our training data, we are ready to extract faces out of the images. In this first example we use the [HAAR feature based cascade classifiers](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_objdetect/py_face_detection/py_face_detection.html) to detect faces. After detection, the faces are resized so that they all have the same shape. If there are multiple faces in an image, we only take the first one.","metadata":{}},{"cell_type":"markdown","source":"About this cascade classifiers it can be said that they are part of a machine learning approach where a lot of images with faces are needed to train the model, since it is very fast compared to the other preprocessors we discuss below. Below, a pretrained model is loaded from a XML file and the detection is mainly done with the method `detectMultiScale`.","metadata":{}},{"cell_type":"code","source":"class HAARPreprocessor():\n    \"\"\"Preprocessing pipeline built around HAAR feature based cascade classifiers. \"\"\"\n    \n    def __init__(self, path, face_size):\n        self.face_size = face_size\n        file_path = os.path.join(path, \"haarcascade_frontalface_default.xml\")\n        if not os.path.exists(file_path): \n            if not os.path.exists(path):\n                os.mkdir(path)\n            self.download_model(file_path)\n        \n        self.classifier = cv2.CascadeClassifier(file_path)\n  \n    def download_model(self, path):\n        url = \"https://raw.githubusercontent.com/opencv/opencv/master/data/\"\\\n            \"haarcascades/haarcascade_frontalface_default.xml\"\n        \n        with request.urlopen(url) as r, open(path, 'wb') as f:\n            f.write(r.read())\n            \n    def detect_faces(self, img):\n        \"\"\"Detect all faces in an image.\"\"\"\n        \n        img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        return self.classifier.detectMultiScale(\n            img_gray,\n            scaleFactor=1.2,\n            minNeighbors=5,\n            minSize=(30, 30),\n            flags=cv2.CASCADE_SCALE_IMAGE\n        )\n        \n    def extract_faces(self, img):\n        \"\"\"Returns all faces (cropped) in an image.\"\"\"\n        \n        faces = self.detect_faces(img)\n\n        # original HAAR img size\n        return [img[y:y+h, x:x+w] for (x, y, w, h) in faces]\n        #return [cut_out_face(img, x, y, w, h) for (x, y, w, h) in faces]\n    \n    def preprocess(self, data_row):\n        faces = self.extract_faces(data_row['img'])\n        \n        # if no faces were found, return None\n        if len(faces) == 0:\n            nan_img = np.empty(self.face_size + (3,))\n            nan_img[:] = np.nan\n            return nan_img\n        \n        # only return the first face\n        return cv2.resize(faces[0], self.face_size, interpolation = cv2.INTER_AREA)\n            \n    def __call__(self, data):\n        return np.stack([self.preprocess(row) for _, row in data.iterrows()]).astype(int)","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:05:26.743690Z","iopub.status.busy":"2023-04-12T14:05:26.742987Z","iopub.status.idle":"2023-04-12T14:05:26.756299Z","shell.execute_reply":"2023-04-12T14:05:26.755074Z","shell.execute_reply.started":"2023-04-12T14:05:26.743651Z"},"papermill":{"duration":0.042776,"end_time":"2021-03-08T07:57:50.834913","exception":false,"start_time":"2021-03-08T07:57:50.792137","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This HAAR preprocessor is now just used for the train data to show visualization examples in the following subsection and how the process of discarding bad images would look like, since better results can be achieved with other face detectors.\n\nEither way, firstly the HAAR preprocessor object has to be created for its later use on the train data. (We save the preprocessed data and use a different face size for the sake of the tutorial on SIFT later on -- the final pipeline for the classifiers will not rely on the HAAR preprocessor)","metadata":{}},{"cell_type":"code","source":"haar_face_size = (100,100)\nhaar_preprocessor = HAARPreprocessor(path = '../../tmp', face_size=haar_face_size)\ntrain_X_HAAR, train_y_HAAR = haar_preprocessor(train), train[\"class\"].values","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:05:29.024983Z","iopub.status.busy":"2023-04-12T14:05:29.024022Z","iopub.status.idle":"2023-04-12T14:05:33.439291Z","shell.execute_reply":"2023-04-12T14:05:33.438294Z","shell.execute_reply.started":"2023-04-12T14:05:29.024930Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Visualization","metadata":{}},{"cell_type":"markdown","source":"Let us visualize the results of the HAAR preprocessor on the basic train data:","metadata":{}},{"cell_type":"code","source":"if SHOW_PREPROCESSOR_PLOTS:\n    plot_image_sequence(train_X_HAAR, imgs_per_row=10)","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:05:42.086479Z","iopub.status.busy":"2023-04-12T14:05:42.085528Z","iopub.status.idle":"2023-04-12T14:05:42.091917Z","shell.execute_reply":"2023-04-12T14:05:42.090624Z","shell.execute_reply.started":"2023-04-12T14:05:42.086440Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, the HAAR detector occasionally fails to detect a face in an image. Hence, we will remove images that had any negative pixel value, since it would mean that there was no detection:","metadata":{}},{"cell_type":"code","source":"# Get the indices of the images with no detection\nindices_to_delete = detect_negatives_images(train_X_HAAR, True)\n\n# Update both the train_X and train_Y arrays deleting those unuseful examples\ntrain_X_HAAR = np.delete(train_X_HAAR, indices_to_delete, axis=0)\ntrain_y_HAAR = np.delete(train_y_HAAR, indices_to_delete, axis=0)\nif SHOW_PREPROCESSOR_PLOTS:\n    plot_image_sequence(train_X_HAAR, imgs_per_row=10)","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:05:52.846523Z","iopub.status.busy":"2023-04-12T14:05:52.845750Z","iopub.status.idle":"2023-04-12T14:05:52.870612Z","shell.execute_reply":"2023-04-12T14:05:52.869635Z","shell.execute_reply.started":"2023-04-12T14:05:52.846489Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that there are still some false detections, due to pieces of hair or a shirt. Hence, we manually remove these false positives and show how the final images would look like. Note that this means that the HAAR preprocessor is not robust or reliable, such that we will consider other face detectors below to process the test data.","metadata":{}},{"cell_type":"code","source":"# Try to remove images that aren't faces or aren't Mila, Jesse, Michael, or Sarah\nindices_not_faces = [5, 17, 22, 23, 27, 33, 38, 47, 58, 59, 62, 67]\ntrain_X_HAAR = np.delete(train_X_HAAR, indices_not_faces, axis=0)\ntrain_y_HAAR = np.delete(train_y_HAAR, indices_not_faces, axis=0)\n# Plot the images\nif SHOW_PREPROCESSOR_PLOTS:\n    plot_image_sequence(train_X_HAAR, imgs_per_row=10)","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:05:56.370386Z","iopub.status.busy":"2023-04-12T14:05:56.369460Z","iopub.status.idle":"2023-04-12T14:05:56.380532Z","shell.execute_reply":"2023-04-12T14:05:56.379409Z","shell.execute_reply.started":"2023-04-12T14:05:56.370330Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, we could also look at the different people by their labels. For instance, we will show the images of Mila:","metadata":{}},{"cell_type":"code","source":"if SHOW_PREPROCESSOR_PLOTS:\n    plot_image_sequence(train_X_HAAR[train_y_HAAR == 2], imgs_per_row=10)","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:06:00.650158Z","iopub.status.busy":"2023-04-12T14:06:00.649598Z","iopub.status.idle":"2023-04-12T14:06:00.656120Z","shell.execute_reply":"2023-04-12T14:06:00.655102Z","shell.execute_reply.started":"2023-04-12T14:06:00.650112Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 0.3.3 MTCNN preprocessor","metadata":{}},{"cell_type":"markdown","source":"The HAAR preprocessing gives a limited performance when it comes to face detection. We hence cannot trust it to process the large test data and give reliable results. Therefore, other possibilities have been explored, and the first of them is a MTCNN preprocessor. MTCNN stands for **MultiTask Cascaded Convolutional Networks**, which is a deep learning approach that adopts a cascaded structure of 3 stages which exploits the correlation between face detection and alignment in unconstrained environments to move past the performance of other methods like the previous one.\n\nIn this section, we look at [this paper](https://arxiv.org/abs/1604.02878) and [this article](https://machinelearningmastery.com/how-to-perform-face-detection-with-classical-and-deep-learning-methods-in-python-with-keras/) for the implementation of the framework. The mtcnn package needs to be installed (which was done at the start of the notebook), and from that, the pipeline is built is a similar fashion to the other one.","metadata":{}},{"cell_type":"code","source":"class MTCNNPreprocessor():\n    \"\"\"Preprocessing pipeline built around MTCNN.\"\"\"\n    \n    def __init__(self, face_size, width_factor = 0.2):\n\n        self.face_size = face_size\n        self.detector = MTCNN()\n        self.width_factor = width_factor\n\n    def detect_faces(self, img):\n        \"\"\"Detect all faces in an image.\"\"\"\n        \n        # Make a copy, since MTCNN plots bounding boxes on top of our images\n        return self.detector.detect_faces(img.copy())\n        \n    def extract_faces(self, img):\n        \"\"\"Returns all faces (cropped) in an image.\"\"\"\n        \n        # Detect the faces\n        faces = self.detect_faces(img)\n        # Boxes contain x, y, w, h of bounding box of each detected face\n        boxes    = [face['box'] for face in faces]  \n        # Nose key gives x and y\n        noses = [face['keypoints']['nose'] for face in faces]\n        \n        # Extract faces\n        extracted_faces = []\n        for i in range(len(faces)):\n            x, y, w, h = boxes[i]\n            center_x, center_y = noses[i]\n            cut_out = cut_out_face(img, x, y, w, h, width_factor = self.width_factor, center_x = center_x, center_y = center_y)\n            extracted_faces.append(cut_out)\n        \n        return extracted_faces\n    \n    def preprocess(self, data_row):\n        faces = self.extract_faces(data_row['img'])\n        \n        # if no faces were found, return None\n        if len(faces) == 0:\n            nan_img = np.empty(self.face_size + (3,))\n            nan_img[:] = np.nan\n            return nan_img\n        \n        # only return the first face, and resize\n        return cv2.resize(faces[0], self.face_size, interpolation = cv2.INTER_AREA)\n            \n    def __call__(self, data):\n        return np.stack([self.preprocess(row) for _, row in data.iterrows()]).astype(int)","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:06:03.799051Z","iopub.status.busy":"2023-04-12T14:06:03.798485Z","iopub.status.idle":"2023-04-12T14:06:03.816117Z","shell.execute_reply":"2023-04-12T14:06:03.814670Z","shell.execute_reply.started":"2023-04-12T14:06:03.798999Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the same way as in the previous case, this MTCNN preprocessor is now just used for the train data to show visualization examples in the following subsection and how the process of discarding bad images would look like, since better results can still be achieved with the last preprocessing technique shown in this notebook. Also, by doing so, the differences between this first two approaches can be seen in the final pictures.","metadata":{}},{"cell_type":"markdown","source":"The first step, as always, is defining the instance of the MTCNN preprocessor object, to use right after on the train data. We use `%%capture` to suppress output of the MTCNN class within this notebook. Note that this preprocessor is slower than the HAAR preprocessor.","metadata":{}},{"cell_type":"code","source":"%%capture\nmtcnn_preprocessor = MTCNNPreprocessor(FACE_SIZE)\ntrain_X_MTCNN, train_y_MTCNN = mtcnn_preprocessor(train), train['class'].values","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:06:05.643157Z","iopub.status.busy":"2023-04-12T14:06:05.642792Z","iopub.status.idle":"2023-04-12T14:07:19.378328Z","shell.execute_reply":"2023-04-12T14:07:19.376958Z","shell.execute_reply.started":"2023-04-12T14:06:05.643123Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The output of MTCNN contains more than just a bounding box around a face, and an example output is shown below in comment. It is a dictionary which, besides the bounding box of the face under the key `box`, also contains a keyword `keypoints`. This is another dictionary containing keys of which the values indicate coordinates of important features of a face, such as locations of the eyes and nose. Our custom function `cut_out_face` allows users to give e.g. the coordinates of the nose, to center this keypoint location across all faces. Such centering is beneficial for instance for PCA, as we will discuss later on.","metadata":{}},{"cell_type":"code","source":"## Example output of detect_faces:\n# {'box': [62, 102, 170, 220],\n#   'confidence': 0.9995554089546204,\n#   'keypoints': {'left_eye': (102, 194),\n#    'right_eye': (187, 191),\n#    'nose': (145, 243),\n#    'mouth_left': (112, 273),\n#    'mouth_right': (183, 269)}},","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:08:24.057533Z","iopub.status.busy":"2023-04-12T14:08:24.056736Z","iopub.status.idle":"2023-04-12T14:08:24.062120Z","shell.execute_reply":"2023-04-12T14:08:24.060872Z","shell.execute_reply.started":"2023-04-12T14:08:24.057494Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Visualization","metadata":{}},{"cell_type":"markdown","source":"We visualize the results of the MTCNN preprocessor on the basic train data:","metadata":{}},{"cell_type":"code","source":"if SHOW_PREPROCESSOR_PLOTS:\n    plot_image_sequence(train_X_MTCNN, imgs_per_row=10)","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:08:27.296166Z","iopub.status.busy":"2023-04-12T14:08:27.295557Z","iopub.status.idle":"2023-04-12T14:08:27.301639Z","shell.execute_reply":"2023-04-12T14:08:27.300627Z","shell.execute_reply.started":"2023-04-12T14:08:27.296117Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And now, in the same way it was done in the HAAR case, the non-detections and some bad results are discarded to get a final cleaned set of images:","metadata":{}},{"cell_type":"code","source":"# Remove some manually detected bad results\nindices_not_faces = [8, 28, 30, 40]\ntrain_X_MTCNN = np.delete(train_X_MTCNN, indices_not_faces, axis=0)\ntrain_y_MTCNN = np.delete(train_y_MTCNN, indices_not_faces, axis=0)\n\n# Discard non-detections\nindices_to_delete = detect_negatives_images(train_X_MTCNN, True)\n\n# Update both the train_X and train_Y arrays deleting those indices\ntrain_X_MTCNN = np.delete(train_X_MTCNN, indices_to_delete, axis=0)\ntrain_y_MTCNN = np.delete(train_y_MTCNN, indices_to_delete, axis=0)\n# Plot image sequence\nif SHOW_PREPROCESSOR_PLOTS:\n    plot_image_sequence(train_X_MTCNN,  imgs_per_row=10)","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:08:32.084317Z","iopub.status.busy":"2023-04-12T14:08:32.083162Z","iopub.status.idle":"2023-04-12T14:08:32.163728Z","shell.execute_reply":"2023-04-12T14:08:32.162591Z","shell.execute_reply.started":"2023-04-12T14:08:32.084262Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below, we will not continue with the MTCNN preprocessor, we delete the variables to clean up memory","metadata":{}},{"cell_type":"code","source":"del train_X_MTCNN\ndel train_y_MTCNN","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:08:35.015721Z","iopub.status.busy":"2023-04-12T14:08:35.014630Z","iopub.status.idle":"2023-04-12T14:08:35.021294Z","shell.execute_reply":"2023-04-12T14:08:35.019964Z","shell.execute_reply.started":"2023-04-12T14:08:35.015681Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 0.3.4 DeepFace preprocessor","metadata":{}},{"cell_type":"markdown","source":"The final face detector we explore is **DeepFace**. To explain how the DeepFace preprocessor works and provide a motivation for its use, let's look at an example. Both HAAR and MTCNN failed on training image with index 49, shown below. The detectors have a more confident detection due to Kristen Stewart in the image. We can exploit the knowledge from the class label, and use a template of Jesse to force our detector to detect Jesse's face instead by relying on a similarity score. Note that we can also prevent this issue by cropping the image such that only the person we wish to detect is in the image, as we did in the beginning of our preprocessing. However, it is clear that the method provided by DeepFace is more robust, and is easily scalable towards larger datasets. ","metadata":{}},{"cell_type":"code","source":"test_image = train.loc[49].img\nif SHOW_PREPROCESSOR_PLOTS:\n    plt.imshow(test_image)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-12T18:05:31.039339Z","iopub.execute_input":"2023-04-12T18:05:31.040053Z","iopub.status.idle":"2023-04-12T18:05:31.046887Z","shell.execute_reply.started":"2023-04-12T18:05:31.040014Z","shell.execute_reply":"2023-04-12T18:05:31.045658Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"We saved a template where Jesse's face is clearly visible and the single face in the picture. We load it in:","metadata":{}},{"cell_type":"code","source":"template_jesse = cv2.imread(os.path.join(template_path, \"template_jesse.png\"))\n# We have to convert to RGB\ntemplate_jesse = template_jesse[...,::-1]","metadata":{"execution":{"iopub.status.busy":"2023-04-12T18:05:34.259419Z","iopub.execute_input":"2023-04-12T18:05:34.260353Z","iopub.status.idle":"2023-04-12T18:05:34.271093Z","shell.execute_reply.started":"2023-04-12T18:05:34.260317Z","shell.execute_reply":"2023-04-12T18:05:34.270038Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"We then use DeepFace's \"verify\" method, which compares two images and verifies that the same person is present in both images. In the example output, we see that DeepFace then gives us two facial areas: one for each image. Since we are not interested in the template, we take the face from the first image. Below, we show an example output in comment. ","metadata":{}},{"cell_type":"code","source":"result = DeepFace.verify(img1_path = test_image, img2_path = template_jesse)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T18:05:35.526134Z","iopub.execute_input":"2023-04-12T18:05:35.526850Z","iopub.status.idle":"2023-04-12T18:05:36.064206Z","shell.execute_reply.started":"2023-04-12T18:05:35.526810Z","shell.execute_reply":"2023-04-12T18:05:36.063143Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"### Example output:\n# {'verified': True,\n#  'distance': 0.11051846168495327,\n#  'threshold': 0.4,\n#  'model': 'VGG-Face',\n#  'detector_backend': 'opencv',\n#  'similarity_metric': 'cosine',\n#  'facial_areas': {'img1': {'x': 65, 'y': 49, 'w': 54, 'h': 54},\n#   'img2': {'x': 116, 'y': 83, 'w': 124, 'h': 124}},\n#  'time': 2.23\n# }","metadata":{"execution":{"iopub.status.busy":"2023-04-12T18:05:37.711785Z","iopub.execute_input":"2023-04-12T18:05:37.712951Z","iopub.status.idle":"2023-04-12T18:05:37.719163Z","shell.execute_reply.started":"2023-04-12T18:05:37.712899Z","shell.execute_reply":"2023-04-12T18:05:37.717987Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"The detected face, when cropped out and plotted  below, is indeed Jesse's face -- we hence were able to detect the desired face in this image due to DeepFace!","metadata":{}},{"cell_type":"code","source":"face = result[\"facial_areas\"][\"img1\"]\nx, y, w, h = face[\"x\"], face[\"y\"], face[\"w\"], face[\"h\"]\n# Plot the face\nif SHOW_PREPROCESSOR_PLOTS:\n    plt.imshow(test_image[y:y+h, x:x+w])\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-12T18:05:39.131398Z","iopub.execute_input":"2023-04-12T18:05:39.132138Z","iopub.status.idle":"2023-04-12T18:05:39.138178Z","shell.execute_reply.started":"2023-04-12T18:05:39.132099Z","shell.execute_reply":"2023-04-12T18:05:39.136679Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"In case we cannot use the template (it is not Jesse or Mila), we have to resort back to a simple face detector to detect faces. There are several detectors implemented in DeepFace, which can be accessed by providing the right name as a string. For instance, the `opencv` detector is simply the HAAR cascade detector we discussed earlier. Also, the MTCNN detector is implemented here. A more state-of-the-art detector based on deep learning is also implemented and selected in the end, known as [RetinaFace](https://arxiv.org/pdf/1905.00641.pdf).\n\nRetinaFace achieves better results compared to both HAAR and MTCNN since it combines face detection, landmark localization and face bounding box regression. Its network is composed of a feature extraction part and convolutional layers that predict the boxes, confidence scores and landmark points. These last ones are also what it is used to align the faces to desired poses. It has been chosen because it achieves higher accuracy and robustness in changing conditions and poses while being only slightly slower in comparison to HAAR and MTCNN.","metadata":{}},{"cell_type":"code","source":"detector_name = \"retinaface\"\ndetector = FaceDetector.build_model(detector_name) #options: opencv, ssd, dlib, mtcnn or retinaface\nobj = FaceDetector.detect_faces(detector, detector_name, test_image)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T18:05:41.720111Z","iopub.execute_input":"2023-04-12T18:05:41.721170Z","iopub.status.idle":"2023-04-12T18:05:44.194040Z","shell.execute_reply.started":"2023-04-12T18:05:41.721120Z","shell.execute_reply":"2023-04-12T18:05:44.192893Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"The output is a list containing the original image, the bounding box of the face, and some score of the detection. Unfortunately, RetinaFace does not give locations of keypoints as MTCNN did. Getting the face out of the image is done as follows:","metadata":{}},{"cell_type":"code","source":"x, y, w, h = obj[0][1]\n# Plot the face\nif SHOW_PREPROCESSOR_PLOTS:\n    plt.imshow(test_image[y:y+h, x:x+w])\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-12T18:05:48.683584Z","iopub.execute_input":"2023-04-12T18:05:48.684322Z","iopub.status.idle":"2023-04-12T18:05:48.689415Z","shell.execute_reply.started":"2023-04-12T18:05:48.684284Z","shell.execute_reply":"2023-04-12T18:05:48.688357Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"The face detector`DEEPPreprocessor` is a preprocessor based on [DeepFace](https://github.com/serengil/deepface). ","metadata":{}},{"cell_type":"code","source":"class DEEPPreprocessor():\n    \n    def __init__(self, face_size, template_jesse_loc =\"template_jesse.png\", template_mila_loc = \"template_mila.png\",\n                 detector_name = \"retinaface\", width_factor = 0.1):\n        \"\"\"Preprocessing pipeline built around DeepFace. \n\n        Args:\n            face_size (tuple[int, int]): Size of the faces for resizing after successful face detection.\n            template_jesse_loc (str, optional): Filename of template image of Jesse. Defaults to \"template_jesse.png\".\n            template_mila_loc (str, optional): Filename of template image of Mila. Defaults to \"template_mila.png\".\n            detector_name (str, optional): Argument required by DeepFace's FaceDetector object. Possible choices are\n            opencv, ssd, dlib, mtcnn or retinaface. Defaults to \"retinaface\".\n            width_factor (float, optional): When cutting out the face in an image, specify the width of the bounding box. Defaults to 0.\n        \"\"\"\n        \n        # Save face size and width factor for bounding boxes\n        self.width_factor = width_factor\n        self.face_size = face_size\n        \n        # Load in Jesse's template\n        self.template_jesse = cv2.imread(os.path.join(template_path, template_jesse_loc))\n        self.template_jesse = self.template_jesse[...,::-1]\n        \n        # Load in Mila's template\n        self.template_mila = cv2.imread(os.path.join(template_path, template_mila_loc))\n        self.template_mila = self.template_mila[...,::-1]\n        \n        # Initialize face detector\n        self.detector_name = detector_name\n        self.detector = FaceDetector.build_model(self.detector_name) \n        \n            \n    def extract_face(self, img, label):\n        \"\"\"Detect and cut out face out of the image. NOTE - unlike HAAR and MTCNN, DeepFace methods detects a single face.\n        This method decides how to detect faces based on the label that is provided, and hence can only be used on training data\n        to improve the face detection and extraction. If the label is 1 or 2, we rely on the templates of Jesse and Mila to \n        improve the performance of the detector (cf. Kristen Stewart example).\n\n        Args:\n            img (np.array): Image.\n            label (int): Class label of training data.\n\n        Returns:\n            np.array: Face cropped out of the image.\n        \"\"\"\n        \n        # Initialize the face bounding box to an empty box (in case no detection, still empty box)\n        x, y, w, h = 0, 0, 0, 0\n        # In case we do NOT have Jesse or Mila, just use face detector\n        if label == 0:\n            # Detect a face\n            face = FaceDetector.detect_faces(detector, self.detector_name, img)\n            # Get the bounding box\n            if face is not None and len(face) > 0:\n                x, y, w, h = face[0][1]\n        else:\n            # In case we know we have to detect Jesse or Mila, make sure we detect them! Use \"verify\"\n            if label == 1:\n                template = self.template_jesse\n            elif label == 2:\n                template = self.template_mila\n            # Use template to verify faces\n            result = DeepFace.verify(img1_path = img, img2_path = template, enforce_detection=False)\n            face = result[\"facial_areas\"][\"img1\"]\n            x, y, w, h = face[\"x\"], face[\"y\"], face[\"w\"], face[\"h\"]\n            \n        return cut_out_face(img, x, y, w, h, width_factor=self.width_factor)\n    \n    def preprocess(self, data_row):\n        \"\"\"Preprocesses the data and extracts the images out of it.\n\n        Args:\n            data_row (pd.DataFrame): Pandas dataframe of training data (images and class labels).\n\n        Returns:\n            _type_: _description_\n        \"\"\"\n        \n        # Detect face\n        face = self.extract_face(data_row['img'], data_row['class'])\n        \n        # No face detected, return nan image                \n        if len(face.flatten()) == 0:\n            nan_img = np.empty(self.face_size + (3,))\n            nan_img[:] = np.nan\n            return nan_img\n        \n        # Resize if face detected\n        return cv2.resize(face, self.face_size, interpolation = cv2.INTER_AREA)\n            \n    def __call__(self, data):\n        return np.stack([self.preprocess(row) for _, row in data.iterrows()]).astype(int)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T18:05:50.345811Z","iopub.execute_input":"2023-04-12T18:05:50.346305Z","iopub.status.idle":"2023-04-12T18:05:50.361685Z","shell.execute_reply.started":"2023-04-12T18:05:50.346271Z","shell.execute_reply":"2023-04-12T18:05:50.360611Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"We now use the DeepFace preprocessor on the training data. Note that this preprocessor is computationally the heaviest of all three discussed!","metadata":{}},{"cell_type":"code","source":"deep_preprocessor = DEEPPreprocessor(FACE_SIZE)\ntrain_X_DEEP, train_y_DEEP = deep_preprocessor(train), train['class'].values\n# Plot\nimg_seq = train_X_DEEP\nif SHOW_PREPROCESSOR_PLOTS:\n    plot_image_sequence(img_seq, imgs_per_row=10)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T18:05:52.052449Z","iopub.execute_input":"2023-04-12T18:05:52.052874Z","iopub.status.idle":"2023-04-12T18:06:38.616891Z","shell.execute_reply.started":"2023-04-12T18:05:52.052837Z","shell.execute_reply":"2023-04-12T18:06:38.615790Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"Again, we delete a few bad faces:","metadata":{}},{"cell_type":"code","source":"indices_not_faces = [40, 65]\ntrain_X_DEEP = np.delete(train_X_DEEP, indices_not_faces, axis=0)\ntrain_y_DEEP = np.delete(train_y_DEEP, indices_not_faces, axis=0)\n# Plot\nif SHOW_PREPROCESSOR_PLOTS:\n    plot_image_sequence(train_X_DEEP, imgs_per_row=10)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T18:06:38.619221Z","iopub.execute_input":"2023-04-12T18:06:38.619670Z","iopub.status.idle":"2023-04-12T18:06:38.654057Z","shell.execute_reply.started":"2023-04-12T18:06:38.619628Z","shell.execute_reply":"2023-04-12T18:06:38.653021Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"Since we are going to use the augmented data, we again delete the above data for the sake of memory/performance:","metadata":{}},{"cell_type":"code","source":"del train_X_DEEP\ndel train_y_DEEP","metadata":{"execution":{"iopub.status.busy":"2023-04-12T18:06:38.655866Z","iopub.execute_input":"2023-04-12T18:06:38.656297Z","iopub.status.idle":"2023-04-12T18:06:38.662239Z","shell.execute_reply.started":"2023-04-12T18:06:38.656244Z","shell.execute_reply":"2023-04-12T18:06:38.661174Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"### 0.3.5 Selection of data to work with","metadata":{}},{"cell_type":"markdown","source":"After the demonstrations above, where we showcased three face detectors and discussed their (dis)advantages, we will now choose to work with DeepFace, as RetinaFace is likely the most effective model out of the three detectors we have discussed. Hence, we hope to maximize the number of (true) detections on the test set with RetinaFace. Since the classifiers will extract features from this preprocessed data, it makes sense to use the same detection strategy for the training set. Hence, below we process the entire augmented training dataset with RetinaFace.","metadata":{}},{"cell_type":"code","source":"deep_preprocessor = DEEPPreprocessor(FACE_SIZE)\ntrain_X_DEEP, train_y_DEEP = deep_preprocessor(train_improved), train_improved['class'].values\n# Plot\nif SHOW_PREPROCESSOR_PLOTS:\n    plot_image_sequence(train_X, imgs_per_row=10)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T18:06:38.664665Z","iopub.execute_input":"2023-04-12T18:06:38.665718Z","iopub.status.idle":"2023-04-12T18:07:16.635108Z","shell.execute_reply.started":"2023-04-12T18:06:38.665681Z","shell.execute_reply":"2023-04-12T18:07:16.633988Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"In the remainder of the tutorial, we will work with this preprocessed training data, so we save it in an additional variable:","metadata":{}},{"cell_type":"code","source":"train_X = train_X_DEEP\ntrain_y = train_y_DEEP","metadata":{"execution":{"iopub.status.busy":"2023-04-12T18:07:16.636750Z","iopub.execute_input":"2023-04-12T18:07:16.637117Z","iopub.status.idle":"2023-04-12T18:07:16.643674Z","shell.execute_reply.started":"2023-04-12T18:07:16.637075Z","shell.execute_reply":"2023-04-12T18:07:16.642619Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"## 0.4. Store Preprocessed data (optional)\n<div class=\"alert alert-block alert-info\">\n<b>NOTE:</b> You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\". Feel free to use this to store intermediary results.\n</div>","metadata":{"papermill":{"duration":0.100995,"end_time":"2021-03-08T07:59:03.904684","exception":false,"start_time":"2021-03-08T07:59:03.803689","status":"completed"},"tags":[]}},{"cell_type":"code","source":"## save preprocessed data\nprep_path = '/kaggle/working/prepped_data/'\nif not os.path.exists(prep_path):\n    os.mkdir(prep_path)\n    \nnp.save(os.path.join(prep_path, 'train_X.npy'), train_X)\nnp.save(os.path.join(prep_path, 'train_y.npy'), train_y)\n# np.save(os.path.join(prep_path, 'test_X.npy'), test_X)","metadata":{"execution":{"iopub.execute_input":"2023-04-12T15:42:46.366182Z","iopub.status.busy":"2023-04-12T15:42:46.365761Z","iopub.status.idle":"2023-04-12T15:42:46.448228Z","shell.execute_reply":"2023-04-12T15:42:46.447079Z","shell.execute_reply.started":"2023-04-12T15:42:46.366146Z"},"papermill":{"duration":0.109823,"end_time":"2021-03-08T07:59:04.11528","exception":false,"start_time":"2021-03-08T07:59:04.005457","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Note*: We have used the DeepFace preprocessor on the test set as well. However, this is computationally very intensive (processing the whole dataset took us around 3 hours of computing), which is why we do not re-do the preproccesing here. Rather, we will import the data and point towards the relevant directory. ","metadata":{}},{"cell_type":"code","source":"test_prep_X_loc = \"/kaggle/input/test-preprocessed/test\"\nprint(f\"Loading preprocessed test data from {test_prep_X_loc}\")","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:49:24.926754Z","iopub.status.busy":"2023-04-12T14:49:24.926353Z","iopub.status.idle":"2023-04-12T14:49:24.932927Z","shell.execute_reply":"2023-04-12T14:49:24.931745Z","shell.execute_reply.started":"2023-04-12T14:49:24.926717Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Feature Representations\n\nNow that we preprocessed the data and extracted the faces out of the provided training data with an effective face detector, the next step is to build lower-dimensional representations, known as **feature representations** of the faces of each of the three classes. These features will be provided to our classifiers later on to be able to infer the class (recognize the face) in a new image.\n\nOur feature extractors will be subclasses of a simple base class that implements the identity function:\n$$\n\\forall x : f(x) = x.\n$$\n","metadata":{"papermill":{"duration":0.100212,"end_time":"2021-03-08T07:59:04.516059","exception":false,"start_time":"2021-03-08T07:59:04.415847","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class IdentityFeatureExtractor:\n    \"\"\"A simple function that returns the input\"\"\"\n    \n    def transform(self, X):\n        return X\n    \n    def __call__(self, X):\n        return self.transform(X)","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:16:22.991443Z","iopub.status.busy":"2023-04-12T14:16:22.991082Z","iopub.status.idle":"2023-04-12T14:16:22.997374Z","shell.execute_reply":"2023-04-12T14:16:22.996161Z","shell.execute_reply.started":"2023-04-12T14:16:22.991410Z"},"papermill":{"duration":0.108781,"end_time":"2021-03-08T07:59:04.725071","exception":false,"start_time":"2021-03-08T07:59:04.61629","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.1. Baseline 1: Scale Invariant Feature Transform (SIFT)","metadata":{"papermill":{"duration":0.134288,"end_time":"2021-03-08T07:59:04.959911","exception":false,"start_time":"2021-03-08T07:59:04.825623","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"The first method to extract features out of images is the **scale invariant feature transform** (SIFT). SIFT is a method of feature extraction that is invariant to various transformations, such as scale, orientation, angle, etc. This makes it a promising method for face recognition as it can extract facial features specific to a person of interest from various images. A successful SIFT implementation is robust (same features are extracted from the same object in different conditions) and discriminative (different image objects can be easily separated from each other in feature space).\n\nIn this section we will implement SIFT using the opencv framework and demonstrate the advantages and disadvantages of these handcrafted features.","metadata":{}},{"cell_type":"code","source":"class SIFTFeatureExtractor(IdentityFeatureExtractor):\n\n    # initialize feature extractor\n    def __init__(self, nFeatures, sigma, nOctaveLayers = 3, contrastThreshold = .04, edgeThreshold=10):\n        self.nFeatures = nFeatures\n        self.sigma = sigma\n        self.allDescriptors = None\n        self.allKeypoints = None\n        self.sift = cv2.SIFT_create(nfeatures=nFeatures, sigma=sigma, nOctaveLayers=nOctaveLayers, contrastThreshold=contrastThreshold, edgeThreshold=edgeThreshold)\n\n    # get keypoints and feature descriptors for each image\n    def detect_and_compute(self, images):\n        all_descriptors = np.zeros((len(images),self.nFeatures, 128))\n        all_keypoints = []\n        # iterate over all images\n        for i in range(len(images)):\n            im = np.array(images[i], dtype='uint8')\n            # convert to grayscale\n            gray= cv2.cvtColor(im,cv2.COLOR_BGR2GRAY)\n            # apply bilateral filter to reduce noise\n            filtered = cv2.bilateralFilter(gray, 3,150,150)\n            # detectAndCompute returns keypoints and feature descriptors\n            kp1, des1 = self.sift.detectAndCompute(filtered, None)\n            (rows,columns) = des1.shape\n            # maintain only nFeatures features for each image\n            if rows>self.nFeatures:\n                des1 = des1[0:self.nFeatures]\n            all_descriptors[i] =np.array(des1)\n            all_keypoints.append(np.array(kp1)[0:self.nFeatures])\n        self.allKeypoints = all_keypoints\n        self.allDescriptors = all_descriptors\n        return all_descriptors, all_keypoints\n\n    # return all matching features between two images\n    def match(self, des1, des2):\n        FLANN_INDEX_KDTREE = 0\n        index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n        search_params = dict(checks=50) # or pass empty dictionary\n        matcher = cv2.FlannBasedMatcher(index_params, search_params)\n        matches = matcher.knnMatch(np.float32(des1), np.float32(des2), k=2)\n        return matches\n\n    # draw the keypoints of an image\n    def drawKeypoints(self, img, keypoints):\n        image = np.zeros((100,100,3))\n        image =cv2.drawKeypoints(img,keypoints,image)\n        plt.imshow(image)\n\n    # draw the matches between two images\n    def drawMatches(self, im1, kp1, des1, im2, kp2, des2):\n        matches = self.match(des1, des2)\n        ratio_thresh = .85\n        good_matches = []\n        for m,n in matches:\n            if m.distance < ratio_thresh * n.distance:\n                good_matches.append([m])\n        im = cv2.drawMatchesKnn(im1, kp1, im2, kp2, good_matches, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n        fig=plt.figure(figsize=(10,10))\n        ax = fig.add_subplot(111)\n        ax.imshow(im)\n        plt.show()\n\n    # filter out the poor matches between two images\n    def getGoodFeatures(self, class_template, desc, kp):\n        good_descriptors = []\n        good_keypoints = []\n        for i,des in enumerate(desc):\n            matches = SIFTExtractor.match(class_template, des)\n            # measure euclidian distance between two matches\n            ratio_thresh = .85\n            for m,n in matches:\n                if m.distance < ratio_thresh * n.distance:\n                    good_descriptors.append(des[m.queryIdx])\n                    good_keypoints.append(kp[i][m.queryIdx])\n        return good_descriptors, good_keypoints","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:16:26.076012Z","iopub.status.busy":"2023-04-12T14:16:26.075130Z","iopub.status.idle":"2023-04-12T14:16:26.093643Z","shell.execute_reply":"2023-04-12T14:16:26.092516Z","shell.execute_reply.started":"2023-04-12T14:16:26.075959Z"},"papermill":{"duration":0.110122,"end_time":"2021-03-08T07:59:05.171171","exception":false,"start_time":"2021-03-08T07:59:05.061049","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.1.1. Extracting Features\n\nHere, we will demonstrate how to use SIFT and interpret the results. First we initialize our SIFT extractor with the parameters `nFeatures=10` and `sigma=.9`. Selecting a value of 10 for `nFeatures` allows our SIFT extractor to limit keypoints to discriminative features such as the contours of the eyes, nose, mouth. The `sigma` parameter represents the sigma of the Gaussian that is applied to the image. Since our images have a weak quality and an additional bilateral filter is applied to each image before extracting features, we reduce this number.\n\nThese values (as well as the optional parameters `contrastTreshold`, `edgeThreshold`, `nOctaveLayers`) can be customized for various recognition tasks and depending on the quality of the input images.","metadata":{"papermill":{"duration":0.100377,"end_time":"2021-03-08T07:59:05.372401","exception":false,"start_time":"2021-03-08T07:59:05.272024","status":"completed"},"tags":[]}},{"cell_type":"code","source":"SIFTExtractor = SIFTFeatureExtractor(nFeatures=10, sigma=.6, nOctaveLayers = 3, contrastThreshold = .04, edgeThreshold=10)","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:16:27.198514Z","iopub.status.busy":"2023-04-12T14:16:27.197761Z","iopub.status.idle":"2023-04-12T14:16:27.203898Z","shell.execute_reply":"2023-04-12T14:16:27.202638Z","shell.execute_reply.started":"2023-04-12T14:16:27.198475Z"},"papermill":{"duration":0.100308,"end_time":"2021-03-08T07:59:05.57403","exception":false,"start_time":"2021-03-08T07:59:05.473722","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As an example, the descriptors and keypoints are extracted from four images. The matches between these four images will be visualized in the following cells. (For the discussion on SIFT, we rely on the HAAR preprocessed data)","metadata":{}},{"cell_type":"code","source":"# get keypoint and feature descriptors from each class of data\ndescriptors_mila, keypoints_mila = SIFTExtractor.detect_and_compute(train_X_HAAR[train_y_HAAR==2])\ndescriptors_jesse, keypoints_jesse = SIFTExtractor.detect_and_compute(train_X_HAAR[train_y_HAAR==1])\ndescriptors_michael_and_sarah, keypoints_michael_and_sarah = SIFTExtractor.detect_and_compute(train_X_HAAR[train_y_HAAR==0])\n\ngray_jesse_1      = cv2.cvtColor(np.uint8(train_X_HAAR[train_y_HAAR==1][0]),cv2.COLOR_BGR2GRAY)\ngray_jesse_2      = cv2.cvtColor(np.uint8(train_X_HAAR[train_y_HAAR==1][1]),cv2.COLOR_BGR2GRAY)\ngray_michael_1 = cv2.cvtColor(np.uint8(train_X_HAAR[train_y_HAAR==0][0]),cv2.COLOR_BGR2GRAY)\ngray_mila_1        = cv2.cvtColor(np.uint8(train_X_HAAR[train_y_HAAR==2][0]),cv2.COLOR_BGR2GRAY)","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:16:29.424066Z","iopub.status.busy":"2023-04-12T14:16:29.423703Z","iopub.status.idle":"2023-04-12T14:16:29.574033Z","shell.execute_reply":"2023-04-12T14:16:29.572988Z","shell.execute_reply.started":"2023-04-12T14:16:29.424033Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we visualize the keypoints on an image of Jesse Eisenberg. Notably, we can see some distinct features extracted from the eyes, mouth, and nostrils. Additional extracted features we can expect from other images in our datasest include the hair and ears.\n\nOne unexpected feature comes from the background of the current image. This could be removed my improving the cropping in the preprocessing of the dataset.","metadata":{}},{"cell_type":"code","source":"SIFTExtractor.drawKeypoints(gray_jesse_1, keypoints_jesse[0])","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:16:30.908773Z","iopub.status.busy":"2023-04-12T14:16:30.908383Z","iopub.status.idle":"2023-04-12T14:16:31.108500Z","shell.execute_reply":"2023-04-12T14:16:31.107479Z","shell.execute_reply.started":"2023-04-12T14:16:30.908738Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's visualize the features of two different images of Jesse Eisenberg. We see that the eye and nostril feature descriptors are identified as matching by the SIFT extractor in both image representations. We do however see that the extractor matches one feature incorrectly. This may be evidence of the feature extractor being too local. Nonetheless, the features are identified as specific to Jesse Eisenberg.","metadata":{}},{"cell_type":"code","source":"SIFTExtractor.drawMatches(gray_jesse_1, keypoints_jesse[0], descriptors_jesse[0], gray_jesse_2, keypoints_jesse[1], descriptors_jesse[1])","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:16:33.356083Z","iopub.status.busy":"2023-04-12T14:16:33.355378Z","iopub.status.idle":"2023-04-12T14:16:33.562701Z","shell.execute_reply":"2023-04-12T14:16:33.561565Z","shell.execute_reply.started":"2023-04-12T14:16:33.356043Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we visualize the features of Jesse Eisenberg and Mila Kunis. We see that no features are identified as matching between these two images. This suggests that the feature representations are discriminative as they are distinguishable between two different people.","metadata":{}},{"cell_type":"code","source":"SIFTExtractor.drawMatches(gray_jesse_1, keypoints_jesse[0], descriptors_jesse[0], gray_mila_1, keypoints_mila[0], descriptors_mila[0])","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:16:35.049183Z","iopub.status.busy":"2023-04-12T14:16:35.048479Z","iopub.status.idle":"2023-04-12T14:16:35.254630Z","shell.execute_reply":"2023-04-12T14:16:35.253619Z","shell.execute_reply.started":"2023-04-12T14:16:35.049142Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A true test of our feature recognition system is comparing the features of two 'lookalikes'. For this we visualize the features of Jesse Eisenberg and Michael Cera. The SIFT extractor matches a feature on Jesse and Michael's eye which shows that the matcher is not as discriminative with lookalikes.","metadata":{}},{"cell_type":"code","source":"SIFTExtractor.drawMatches(gray_jesse_1, keypoints_jesse[0], descriptors_jesse[0], gray_michael_1, keypoints_michael_and_sarah[0], descriptors_michael_and_sarah[0])","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:16:36.768881Z","iopub.status.busy":"2023-04-12T14:16:36.767385Z","iopub.status.idle":"2023-04-12T14:16:36.972559Z","shell.execute_reply":"2023-04-12T14:16:36.971555Z","shell.execute_reply.started":"2023-04-12T14:16:36.768830Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.1.2. T-SNE Plot\n\nThe features we extracted with SIFT are still located in a high-dimensional space, such that it is hard to visualize the feature representations of our training images. In order to visualize them, we have to postprocess the features with a **T-SNE** or **t-Distributed Stochastic Neighbor Embedding**, which is a method used to visualie high dimensional data in fewer dimensions.\n\nHere we visualize the good descriptors of Jesse Eisenberg and Mila Kunis. A descriptor is deemed good if it is similar to other descriptors in the same locality of the image. Similarity here is measured with euclidean distance.\n\nIn this section we will use the t-SNE framework from sklearn. We first initialize a TSNE object with the parameters `n_components=2`, `perplexity=20`, `early_exaggeration=20` and the optional parameters `learning_rate='auto'`, `init='random'` and  `n_iter=2000`.\n\nWe choose 2 for the number of components because it is more intuitive for visualization purposes. Perplexity is generally a value between 5 and 50 that represents the balance between local and global features in our feature space. We choose 50 for this value as we estimate 50 near neighbors in each class. A value of 20 is chosen for `early_exaggeration` to accentuate the distance between classes.\n\nT-SNE uses a factor of randomization to create the plot, thus each run may result in a slightly different distribution. Nonetheless we can see that the features for Jesse and Mila form two distinct classes. However, in some runs the distance between these two classes is not very big. This is expected as the feature representations for facial features may be similar even from person to person.","metadata":{"papermill":{"duration":0.100596,"end_time":"2021-03-08T07:59:05.775686","exception":false,"start_time":"2021-03-08T07:59:05.67509","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Get the 'good' descriptors of all images of mila and jesse. A descriptor is deemed good if it is similar to other descriptors in the same locality of the image where similarity is measured with euclidean distance.\ngood_mila_descriptors, _ = SIFTExtractor.getGoodFeatures(descriptors_mila[0], descriptors_mila, keypoints_mila)\ngood_jesse_descriptors, _ = SIFTExtractor.getGoodFeatures(descriptors_jesse[0], descriptors_jesse, keypoints_jesse)\n\n# plot the tsne\ntsne = TSNE(n_components=2, learning_rate='auto', init='random', perplexity=50, n_iter=2000, verbose=1, early_exaggeration=20)\nz_jesse = tsne.fit_transform(np.array(good_jesse_descriptors))\nz_mila = tsne.fit_transform(np.array(good_mila_descriptors))\n\nsns.scatterplot(x=z_jesse[:,0], y=z_jesse[:,1], label='jesse')\nsns.scatterplot(x=z_mila[:,0], y=z_mila[:,1], label='mila')\nplt.grid()\nplt.axhline(0, color=\"black\")\nplt.axvline(0, color=\"black\")\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:16:59.427569Z","iopub.status.busy":"2023-04-12T14:16:59.426856Z","iopub.status.idle":"2023-04-12T14:17:00.546487Z","shell.execute_reply":"2023-04-12T14:17:00.545544Z","shell.execute_reply.started":"2023-04-12T14:16:59.427501Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.1.3. Discussion\nIn this section we observed that the handcrafted SIFT features are discriminative. From both the visualized matching keypoints and the TSNE plot we observe that SIFT effectively separates features of two different people, namely Jesse Eisenberg and Mila Kunis.\n\nIn regard to robustness, the handcrafted features perform less effectively in this domain. While the SIFT extractor generally extracts a few features effectively (such as the eye and nostril features in the first image of Jesse Eisenberg), only a few 'good' features are extracted for each image. A more robust system would extract features from the eyes, nose, mouth, and ears for each image regardless of the image conditions. The system can be made more robust by improving the preprocessing of images before extracting features. While the SIFT extractor applies a Gaussian to images before grabbing features, additional steps such as normalizing contrast and brightness could improve the definition of features.\n\nCompared to the previous grabbing task in the individual assignment, the SIFT extractor applied in facial recognition must be more local. What is meant by this is that distinguishing features from person to person is a more difficult task than object recognition seen in the previous assignment. Because of this the SIFT extractor needs to be well tuned to distinguish the intricacies of facial features.","metadata":{"_kg_hide-input":true}},{"cell_type":"markdown","source":"## 1.2. Baseline 2: PCA feature extractor","metadata":{"papermill":{"duration":0.101426,"end_time":"2021-03-08T07:59:05.978236","exception":false,"start_time":"2021-03-08T07:59:05.87681","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"The second method to extract features out of image data that we explore is principal component analysis. __Principal Component Analysis__ (PCA) is a technique used to extract highly variant components (set of points) from data. In our case, we will apply it to faces in order to extract those components that make up the variability of the faces. Ideally, we would discretize or isolate components containing facial features that can be later used to discriminate the faces in our dataset. Most of the work in applying this method relies in picking informative components and aligning the faces to a reference point to reduce variance generated only based on the location of the face.\n\nFor PCA, we first convert each image in our training set into a one-dimensional array by flattening the images. If color images are used, we flatten each color channel and concatenate them. For simplicity, we discuss the case of performing PCA on square grasycale images, each having the same size $K\\times K$ (with, by default, $224\\times 224$ for our training set). After flattening, we hence have a matrix $M$, often called the __data matrix__ of size $Z \\times K^2$, with $Z$ the number of training examples. The columns (flattened pixels) then represent random variables, while the rows indicate different samples of those random variables. In our case, the matrix hence has size $80 \\times 10 \\ 000$. Given this data matrix, we zero-mean the dataset. That is, we compute the mean of each column and subtract it from all training examples. The reason behind centering the data is that the covariance matrix is sensitive to the mean of the data: without centering, the covariance matrix will reflect the location of the mean of the data (the bias in the training data) as well as its variability, which makes for a worse PCA performance.\n\nThere are then two ways to proceed with PCA. The first option is to compute the covariance matrix of this data matrix $M$. This will give a covariance matrix of size $K^2 \\times K^2$, of which we compute the eigenvalue decomposition (EVD). The principal components are then determined by the largest eigenvalues and their corresponding eigenvectors of this covariance matrix, since they indicate the directions of highest variance. By keeping $q$ principal components, we compute a projection matrix of size $q \\times K^2$ which stores the corresponding eigenvectors as columns. Given a flattened image $x$ with shape $K^2 \\times 1$, we can compute a lower-dimensional representation $z = W\\cdot x$, and $z$ is often referred to as the __feature vector__. The time complexity of computing the EVD on a $n\\times n$ matrix is $\\mathcal{O}(n^3)$.\n\nThe other approach is to use **singular value decomposition** (SVD). It generalizes the concept of eigenvalue decomposition, which can only be applied to square matrices, to matrices of any shape. Similar to the EVD case, here we keep the $q$ largest singular values and their corresponding vectors as the principal components. The time complexity of computing the SVD on a $m\\times n$ matrix is $\\mathcal{O}\\left( \\text{min}\\left( mn^2, m^2 n \\right) \\right)$.\n\nIn our case, the matrix of interest has shape $80 \\times 10 \\ 000$. Due to this dimensionality, computing the SVD of this matrix is much more efficient than the EVD. During our experiments, we noted that computing the EVD on the covariance matrix could easily take more than 10 minutes, whereas the SVD takes only a few seconds. We did not see any significant advantage for the EVD in terms of performance. As such, we decided to stick to the SVD in this implementation. Our implementation makes use of scikit-learn's PCA, which in turn calls numpy's SVD optimised function.\n\nOne challenge with PCA is choosing the number of principal components. One way to choose this value is by looking at the eigenvalues/singular values $\\lambda_i$, $i = 1, ..., N$ of the data matrix, ordered in decreasing magnitude. (For EVD, $N$ equals $K^2$, while for SVD, $N$ equals $Z$). We can then decide the number of principal components $q$ by requiring that these components explain a certain percentage $p$ of the variance in the dataset, *i.e.* choose $q$ such that\n\\begin{equation*}  \n    \\frac{\\sum_{i=1}^q \\lambda_i}{\\sum_{i=1}^N \\lambda_i} \\geq p \\, .\n\\end{equation*}  \nAnother approach could be to investigate the average reconstruction loss on the training images. In the end, the number of principal components is another hyperparameter of the classifier, and one needs to carefully tune this hyperparameter, such as via a knee plot. \n\nLastly, although already mentioned briefly, it is worth explaining the decision behind doing the mean subtraction. It is a common preprocessing step in PCA, since it improves the resulting covariance matrix, due to its sensitivity to the mean of the data. Commonalities on every face are not informative, so with a minimum information loss we can focus on the variations around the mean. In addition to that, interpreting the meaning of the principal components gets facilitated, since they get aligned with the directions of maximum variation. At the same time, although talking about PCA on square grayscale images where it might not be that powerful, mean subtraction can remove some bias due to lighting as it would subtract the mean pixel intensity making it easier to focus on the relevant variations.","metadata":{}},{"cell_type":"code","source":"class PCAFeatureExtractor(IdentityFeatureExtractor):\n    \"\"\"\n    PCA feature extractor which, given training data, finds the ideal set of principal components. Inspired by assignment 3 of the artificial neural networks course.\n    \"\"\"\n    \n    def __init__(self, n_components=20, use_color=True):\n        \"\"\"Initialization.\n\n        Args:\n            n_components (int/float, optional): Number of components to keep. Either an integer or a float. When a float, this is the percentage\n            of the total variance that has to be explained by the principal components. Defaults to 20.\n            use_color (bool, optional): Whether to use all color channels for PCA, or to work with grayscale images. Defaults to True.\n        \"\"\"\n\n        self.n_components = n_components\n        # ^ the number of principal components to be computed\n        self.model = None\n        # ^ the PCA model, will be initialized in fit\n        self.meanface = None\n        # ^ the mean of all the input training data\n        self.use_color = use_color\n        # ^ whether we train PCA on color images or on grayscale images\n\n        # We will save the original shape of the input data for convenience, used inside this class\n        self.width     = None\n        self.height    = None\n        self.shape     = None\n\n    def preprocess_data(self, X):\n        \"\"\"Preprocess the images, such as converting to grayscale and flattening.\n\n        Args:\n            X (np.array): Data matrix containing several images of the same shape.\n\n        Returns:\n            None: no return\n        \"\"\"\n        # Save the original shape for later on\n        self.shape = X.shape[1:]\n        \n        # In case we use grayscale, drop color channels\n        if not self.use_color:\n            self.shape = self.shape[:-1]\n        self.width, self.height = self.shape[0], self.shape[1]\n\n        if self.use_color:\n            self.data_matrix = np.array([img.flatten() for img in X])\n        else:\n            # Convert images to grayscale\n            X_gray = np.mean(X, axis=3)\n            # Flatten images\n            self.data_matrix = np.array([img.flatten() for img in X_gray])\n\n    def fit(self, X):\n        \"\"\"Fit the PCA on the training data.\n\n        Args:\n            X (np.array): The training data, consisting of several images of the same shape.\n        \"\"\"\n        self.model = PCA(self.n_components, svd_solver=\"full\", whiten=True)\n        self.preprocess_data(X)\n        self.model.fit(self.data_matrix)\n        # Also save the mean face as a 2D image, in color or grayscale\n        self.meanface = rescale_intensity(self.model.mean_.reshape(self.shape), out_range=(0, 255)).astype(np.uint8)\n        # Save the eigenfaces for convenience of plotting later on:\n        self.eigenfaces = np.array([rescale_intensity(face.reshape(self.shape), out_range=(0, 255)).astype(np.uint8) for face in self.model.components_])\n\n    def transform(self, X):\n        \"\"\"Compute feature vector of the given sequence of images according to the fitted PCA, i.e. project on principal components.\n        NOTE has to be an array of images, so for a single image, put it in an array of one element\n\n        Args:\n            X (np.array): Sequence of images to be converted to their latent/feature representation.\n\n        Returns:\n            np.array: Features of the sequence of provided images.\n        \"\"\"\n        \n        # Simply call the transform of scikit-learn's PCA, but reshape the matrix in desired shape\n        X = np.array([img.flatten() for img in X])\n\n        return self.model.transform(X)\n\n    def inverse_transform(self, X):\n        \"\"\"Reconstruct images (i.e. convert back to their original shape) based on their latent feature representation.\n        NOTE has to be an array of images, so for a single image, put it in an array of one element\n        Args:\n            X (np.array): Feature representation of the images.\n\n        Returns:\n            np.array: Reconstructed images.\n        \"\"\"\n        X = np.array([img.flatten() for img in X])\n\n        # Go from the latent space back to original space\n        reconstructed = self.model.inverse_transform(X)\n        # Reshape back into a 2D image (works for both color or gray)\n        reconstructed = np.array([img.reshape(self.shape) for img in reconstructed])\n        # Normalize the values:\n        reconstructed = rescale_intensity(reconstructed, out_range=(0, 255)).astype(np.uint8)\n\n        return reconstructed ","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:17:08.331784Z","iopub.status.busy":"2023-04-12T14:17:08.331399Z","iopub.status.idle":"2023-04-12T14:17:08.346024Z","shell.execute_reply":"2023-04-12T14:17:08.344943Z","shell.execute_reply.started":"2023-04-12T14:17:08.331748Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To demonstrate our discussion above on \"explained variance\" as determined by the size of the eigenvalues/singular values, we can plot the singular values, ordered according to decreasing magnitude. This is stored in scikit-learn's PCA object (for more information, consult [the documentation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)).","metadata":{}},{"cell_type":"code","source":"# Initialize PCA, but DO NOT set number of components to have explained variance\npca = PCAFeatureExtractor(n_components=None)\npca.fit(train_X)","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:17:10.079149Z","iopub.status.busy":"2023-04-12T14:17:10.078057Z","iopub.status.idle":"2023-04-12T14:17:13.237847Z","shell.execute_reply":"2023-04-12T14:17:13.236795Z","shell.execute_reply.started":"2023-04-12T14:17:10.079097Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if SHOW_PLOTS:\n    plt.plot(pca.model.singular_values_, \"-o\", color=\"red\")\n    plt.xlabel(\"Singular value index\")\n    plt.ylabel(\"Singular value magnitude\")\n    plt.title(\"Singular values for training data\")\n    plt.grid()\n    plt.axhline(0, color=\"black\")\n    plt.show()","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:17:30.225422Z","iopub.status.busy":"2023-04-12T14:17:30.224856Z","iopub.status.idle":"2023-04-12T14:17:30.432762Z","shell.execute_reply":"2023-04-12T14:17:30.431788Z","shell.execute_reply.started":"2023-04-12T14:17:30.225385Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is only one singular value that is equal to zero in this case. We can also plot the ratio of the cumulative sum of these singular values, which gives the explained variance (see equation above):","metadata":{}},{"cell_type":"code","source":"if SHOW_PLOTS:\n    plt.figure(figsize = (10, 5))\n    # Get the cumulative sum of the explained variance\n    cumsum = np.cumsum(pca.model.explained_variance_ratio_)\n\n    # Plot the cumulative sum\n    plt.plot([i+1 for i in range(len(cumsum))], cumsum, \"-o\", color=\"red\", zorder=100)\n\n    # For visualization, determine when we have 95% explained variance\n    for i, value in enumerate(cumsum):\n        if value >= 0.95:\n            break\n    plt.axhline(0.95, color=\"black\")\n    plt.axvline(i + 1, color=\"black\", label = \"95% explained variance\")\n    plt.xlabel(\"Singular value index\")\n    plt.ylabel(\"Singular value magnitude\")\n    plt.title(\"Singular values for training data\")\n    plt.grid()\n    plt.legend(loc=\"lower right\")\n    plt.show()","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:18:02.800367Z","iopub.status.busy":"2023-04-12T14:18:02.799745Z","iopub.status.idle":"2023-04-12T14:18:03.036534Z","shell.execute_reply":"2023-04-12T14:18:03.035529Z","shell.execute_reply.started":"2023-04-12T14:18:02.800329Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the discussion below, we will initialize a default PCA feature extractor which uses 20 principal components. We will work in color for better visualizations. ","metadata":{}},{"cell_type":"code","source":"pca = PCAFeatureExtractor()\npca.fit(train_X)","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:18:06.046910Z","iopub.status.busy":"2023-04-12T14:18:06.045861Z","iopub.status.idle":"2023-04-12T14:18:09.007156Z","shell.execute_reply":"2023-04-12T14:18:09.005673Z","shell.execute_reply.started":"2023-04-12T14:18:06.046863Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.2.1. Eigenface Plots","metadata":{"papermill":{"duration":0.100881,"end_time":"2021-03-08T07:59:06.392861","exception":false,"start_time":"2021-03-08T07:59:06.29198","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"While fitting our PCA feature extractor to the training data, we also saved the __mean face__. Remember that PCA flattens the images into arrays of size $K^2$, and views the pixels as random variables. We can hence take the sample mean of each pixel over our training set, and end up with an array containing the mean of all pixels. By reshaping this mean array into an image of size $K\\times K$, we can reconstruct this average as an image, and this average is called the mean face: it gives an average representation of a face, according to our training data.","metadata":{}},{"cell_type":"code","source":"if SHOW_PLOTS:\n    plt.imshow(pca.meanface, cmap=\"Greys_r\")\n    plt.title(\"Mean face from training data\")\n    plt.show()","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:18:19.019775Z","iopub.status.busy":"2023-04-12T14:18:19.019307Z","iopub.status.idle":"2023-04-12T14:18:19.388612Z","shell.execute_reply":"2023-04-12T14:18:19.387369Z","shell.execute_reply.started":"2023-04-12T14:18:19.019734Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By fitting the PCA feature extractor, we keep a certain number of \"principal components\" which are either eigenvectors of the covariance matrix or vectors corresponding to singular values, according to the decomposition used in the PCA. Similar to the mean vector, we can reshape these eigenvectors into an image of size $K\\times K$ and visualize them. When the training data consists of faces, these eigenvectors are often referred to as __eigenfaces__, since (when visualized) they resemble faces. These faces (vectors) are used to project images of faces onto a lower-dimensional representation, and due to their order, they maximize the amount of information they retain of the original faces. Below, we plot some of these eigenfaces, in order of decreasing singular values (hence, most important to least important).","metadata":{}},{"cell_type":"code","source":"nb_eigenfaces_to_show = pca.n_components\nnb_per_row = 10\nif SHOW_PLOTS:\n    plot_image_sequence(pca.eigenfaces[0:nb_eigenfaces_to_show], imgs_per_row=nb_per_row)","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:18:23.735473Z","iopub.status.busy":"2023-04-12T14:18:23.734875Z","iopub.status.idle":"2023-04-12T14:18:31.175069Z","shell.execute_reply":"2023-04-12T14:18:31.173662Z","shell.execute_reply.started":"2023-04-12T14:18:23.735434Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.2.2. Feature Space Plots","metadata":{"papermill":{"duration":0.101263,"end_time":"2021-03-08T07:59:06.797448","exception":false,"start_time":"2021-03-08T07:59:06.696185","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Since the projection of a data point by PCA on a lower-dimensional representation is reversible, we can compare an original image with its \"reconstruction\". That is, we project an image on its lower-dimensional representation, and apply the reverse transformation to get back an image of the original size. This can essentially be seen as a compression or reconstruction, as the new image is created from less information. The number of principal components determines the quality of the reconstructed image. Below, we show an example of such a reconstruction using 20 principal components.","metadata":{}},{"cell_type":"code","source":"# Take an example face to reconstruct\nexample_face = train_X[0]\n# If we don't use color, convert to grayscale\nif not pca.use_color:\n    example_face = np.mean(example_face, axis=2)\n\n# Plot the original face\nif SHOW_PLOTS:\n    fig, (ax0, ax1) = plt.subplots(1, 2)\n    ax0.imshow(example_face.reshape(pca.shape), cmap=\"Greys_r\")\n    ax0.set_title(\"Original face\")\n    # Get reconstruction: first, project onto lower dimensonional repr\n    reduced = pca.transform([example_face])\n    # Then, reverse projection and reconstruct original\n    reconstructed = pca.inverse_transform([reduced])[0]\n    ax1.imshow(reconstructed, cmap=\"Greys_r\")\n    ax1.set_title(f\"Reconstructed face (q = {pca.n_components})\")\n\n    plt.show()","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:19:16.252447Z","iopub.status.busy":"2023-04-12T14:19:16.251958Z","iopub.status.idle":"2023-04-12T14:19:16.697496Z","shell.execute_reply":"2023-04-12T14:19:16.696440Z","shell.execute_reply.started":"2023-04-12T14:19:16.252402Z"},"papermill":{"duration":0.101801,"end_time":"2021-03-08T07:59:07.000598","exception":false,"start_time":"2021-03-08T07:59:06.898797","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As mentioned, the quality of this reconstruction depends on the amount of principal components to keep. Above, there are clear deviations between the original and reconstructed images. Below, we vary over the amount of principal components, below denoted by $q$, we use in the representation. This clearly shows that, while we clearly see only the mean face for low $q$, the reconstruction gradually uses more information of the face of interest, until the final images, with a high value of $q$, show hardly any difference with the original image. ","metadata":{}},{"cell_type":"code","source":"# Specify which components we are going to look at\nmax_nb_components = 50\nstep_size = 5\nn_components_list = np.arange(step_size, max_nb_components+1,step_size)\nnb_per_row = 5\n\nif SHOW_PLOTS:\n    fig, axs = plt.subplots(len(n_components_list)//nb_per_row, nb_per_row, figsize = (15, 10))\n    for i, n_components in enumerate(n_components_list):\n        # Get the current axis\n        ax = axs[i // nb_per_row, i % nb_per_row]\n        # Set number of principal components and recompute the projection matrix\n        pca.n_components = n_components\n        pca.fit(train_X)\n        # Compute the reconstruction\n        reconstructed = pca.inverse_transform(pca.transform([example_face]))[0]\n        # Plot it\n        ax.imshow(reconstructed, cmap=\"Greys_r\")\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.set_title(f\"q = {n_components}\")\n\n    plt.show()","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:23:32.772426Z","iopub.status.busy":"2023-04-12T14:23:32.771447Z","iopub.status.idle":"2023-04-12T14:24:05.862382Z","shell.execute_reply":"2023-04-12T14:24:05.861519Z","shell.execute_reply.started":"2023-04-12T14:23:32.772385Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Of course, while we visually see that the difference between the original image and its reconstruction decreases, we want to have a method of quantifying this difference. One possible measure between the two images is the **root-mean-square deviation** (RMSD) between the two images, viewed as two arrays $\\boldsymbol{x}$ and $\\hat{\\boldsymbol{x}}$ of size $K^2$. The RMSD measure is then defined as\n\\begin{equation*}\n    RMSD(\\boldsymbol{x}, \\hat{\\boldsymbol{x}}) = \\sqrt{\\frac{\\sum_{i=1}^{K^2} (x_i - \\hat{x}_i)}{K^2}}\n\\end{equation*}","metadata":{}},{"cell_type":"code","source":"def rmsd(x: np.array, x_hat: np.array):\n    \"\"\"\n    Computes root mean square deviation between two Numpy arrays.\n    \"\"\"\n\n    return np.sqrt(np.mean((x - x_hat)**2))","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:24:45.543843Z","iopub.status.busy":"2023-04-12T14:24:45.543005Z","iopub.status.idle":"2023-04-12T14:24:45.550654Z","shell.execute_reply":"2023-04-12T14:24:45.549414Z","shell.execute_reply.started":"2023-04-12T14:24:45.543799Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we will again iterate over the number of principal components $q$ we keep in the PCA, and compute for each value of $q$ the average RMSD on the training set. This could give us insights into how much information is being retained by the PCA projection, and could *e.g.* inform us on which value of $q$ would be the most ideal to use in our classification pipeline.","metadata":{}},{"cell_type":"code","source":"# Save the RMSD errors in a list\nerrors = []\n\n# Check whether we train with grayscale or color images\nif pca.use_color:\n    original_data = train_X\nelse:\n    # If we use grayscale, convert all the training images to grayscale as well\n    original_data = np.array([np.mean(train_face, axis=2) for train_face in train_X])\n\nfor i in tqdm(range(len(n_components_list))):\n    n_components = n_components_list[i]\n    # Set number of principal components and recompute the projection matrix\n    pca.n_components = n_components\n    pca.fit(train_X)\n    # Compute the reconstruction\n    reconstructed_faces = pca.inverse_transform(pca.transform(original_data))\n    error = np.mean([rmsd(original_data, reconstructed_faces)])\n    # Append to list, make sure to reshape\n    errors.append(error)\n\nif SHOW_PLOTS:\n    plt.plot(n_components_list, errors, '-o', color='red')\n    plt.ylabel(\"RMSD\")\n    plt.xlabel(\"Number of components\")\n    plt.title(\"Reconstruction error for varying number of principal components\")\n    plt.grid()\n    plt.show()","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:25:43.380464Z","iopub.status.busy":"2023-04-12T14:25:43.380019Z","iopub.status.idle":"2023-04-12T14:26:21.859639Z","shell.execute_reply":"2023-04-12T14:26:21.858513Z","shell.execute_reply.started":"2023-04-12T14:25:43.380423Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can use this information to choose the number of principal components to keep for the classifier later on. For instance, we can consider the \"gain\" in RMSD when we increase the number of principal components to keep. Below, we plot for each value of $q$ the difference between the RMSD value with $q$ components and $q+1$ components. Together with the previous plot, we see that we have the best results for $q=40$ or $q=50$, so it would make sense to try different values of $q$ in the classification pipeline later on to tune this hyperparameter. We expect that, while the reconstruction loss goes steadily down for increasing number of principal components, having simpler features can be beneficial for the classifiers.","metadata":{}},{"cell_type":"code","source":"errors = np.array(errors)\nnext_errors = np.roll(errors, -1)\ndifferences = errors - next_errors\nif SHOW_PLOTS:\n    plt.plot([i+1 for i in range(len(differences[:-1]))], differences[:-1], '-o', color='red')\n    plt.ylabel(\"RMSD gain\")\n    plt.xlabel(\"Number of components\")\n    plt.title(\"RMSD gain for varying number of components\")\n    plt.axhline(0, color='black')\n    plt.grid()\n    plt.show()","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:26:26.911005Z","iopub.status.busy":"2023-04-12T14:26:26.910079Z","iopub.status.idle":"2023-04-12T14:26:27.141438Z","shell.execute_reply":"2023-04-12T14:26:27.140513Z","shell.execute_reply.started":"2023-04-12T14:26:26.910950Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A quantitative measure such as the RMSD is also a great way to compare our different face detectors and choose which preprocessed data is more suited for our use case here. It can also be used to tune the detectors: recall for instance that we defined an auxiliary function to modify the bounding boxes around faces: the hyperparameters involved in the detections can be tuned by their reconstruction error. However, this is beyond the scope of this tutorial.","metadata":{}},{"cell_type":"markdown","source":"#### Face-Feature Plots","metadata":{}},{"cell_type":"markdown","source":"Another great aspect about PCA is that we can create so-called **face-feature plots**, which can also be seen for instance in [this tutorial](https://sandipanweb.wordpress.com/2018/01/06/eigenfaces-and-a-simple-face-detector-with-pca-svd-in-python/). In a face-feature plot, we plot the different faces of the training set at locations that are determined by their features obtained from the PCA projection. For visualization purposes, we therefore restrict ourselves to $q = 2$, such that there are only two eigenfaces of interest. Applying the PCA projection on each image then gives us a 2D representation of the faces, which determine the location in the grid where we plot each image. ","metadata":{}},{"cell_type":"code","source":"pca.n_components = 2\npca.fit(train_X)","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:26:32.092675Z","iopub.status.busy":"2023-04-12T14:26:32.092285Z","iopub.status.idle":"2023-04-12T14:26:34.955975Z","shell.execute_reply":"2023-04-12T14:26:34.954445Z","shell.execute_reply.started":"2023-04-12T14:26:32.092639Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if SHOW_PLOTS:\n    fig, ax = plt.subplots(figsize = (10,10))\n    # Parameters to play with to increase/decrease the boxes of the faces in the plot\n    delta_image = 0.25\n    delta_plot = 0.5\n    # Obtain the latent representations, i.e. after projecting with the PCA\n    latent_reps = np.array([pca.transform([image]).flatten() for image in original_data])\n\n    # Plot each image at their latent representation\n    for i in range(train_X.shape[0]):\n        image = train_X[i]\n        latent = latent_reps[i]\n        ax.imshow(image, interpolation='nearest', extent=(latent[0]-delta_image, latent[0]+delta_image, latent[1]-delta_image, latent[1]+delta_image), zorder=100)\n\n    # Add annotations\n    plt.xlim(latent_reps[:, 0].min()-delta_plot, latent_reps[:, 0].max()+delta_plot)\n    plt.ylim(latent_reps[:, 1].min()-delta_plot, latent_reps[:, 1].max()+delta_plot)\n    plt.xlabel('PC1')\n    plt.ylabel('PC2')\n    plt.title('Face-feature plot')\n    plt.axhline(0, color=\"black\")\n    plt.axvline(0, color=\"black\")\n    plt.grid()\n    plt.show()","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:26:35.840194Z","iopub.status.busy":"2023-04-12T14:26:35.839836Z","iopub.status.idle":"2023-04-12T14:26:37.180123Z","shell.execute_reply":"2023-04-12T14:26:37.177412Z","shell.execute_reply.started":"2023-04-12T14:26:35.840161Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Instead of plotting the faces, it is perhaps more informative to plot the class labels of the faces we extracted. This is done in the plot below, where black stands for class label 0, red for class label 1 and blue for class label 2.","metadata":{}},{"cell_type":"code","source":"if SHOW_PLOTS:\n    fig = plt.figure(figsize = (10,10))\n    # Parameters to play with to increase/decrease the plot window\n    delta_plot = 0.5\n    # Obtain the latent representations, i.e. after projecting with the PCA\n    latent_reps = np.array([pca.transform([image]).flatten() for image in original_data])\n\n    # Use colors instead of the faces\n    colors = [\"black\", \"red\", \"blue\"]\n\n    for i in range(original_data.shape[0]):\n        image = original_data[i]\n        latent = latent_reps[i]\n        plt.scatter(latent[0], latent[1], color = colors[train_y[i]])\n\n    # Annotate plot\n    plt.xlim(latent_reps[:, 0].min()-delta_plot, latent_reps[:, 0].max()+delta_plot)\n    plt.ylim(latent_reps[:, 1].min()-delta_plot, latent_reps[:, 1].max()+delta_plot)\n    plt.xlabel('PC1')\n    plt.ylabel('PC2')\n    plt.title('Class labels plotted in PCA space')\n    plt.axhline(0, color=\"black\")\n    plt.axvline(0, color=\"black\")\n    plt.grid()\n    plt.show()","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:26:44.288297Z","iopub.status.busy":"2023-04-12T14:26:44.287925Z","iopub.status.idle":"2023-04-12T14:26:45.856701Z","shell.execute_reply":"2023-04-12T14:26:45.855584Z","shell.execute_reply.started":"2023-04-12T14:26:44.288262Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Apart from some data points, we see that the red and blue classes are well separated. However, the lookalikes, denoted by the black dots, are spread throughout the PCA space, which is expected since they likely have very similar features as Jesse and Mila. However, we have take into account that this is a PCA representation based on only 2 principal components, which is very low and hence we expect the separation between classes to be quite poor. By increasing the amount of principal components, we can make the PCA features more discriminative.","metadata":{}},{"cell_type":"markdown","source":"### 1.2.3. Discussion","metadata":{"papermill":{"duration":0.102099,"end_time":"2021-03-08T07:59:07.204783","exception":false,"start_time":"2021-03-08T07:59:07.102684","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"As a quick recap, in this section we have seen how PCA works in practice, which options it provides to choose from depending on the traning data and what results it gives at every step of the process. One of the most critical choices that has to be made is the number of principal components, and for that, the root-mean-square deviation was used. This measure gives us a tool to calculate for every iteration on the number of principal components the *loss* of the reconstruction. This can guide the choice of the number of principal components, although one should try to get a balance between simplicity of the features and complexity in representing the information in the faces, for which the RMSD measure alone and considering the reconstruction loss is probably insufficient.\n\nAdditionally, throughout the whole section different plots and graphs have been shown at each step to better understand how the components of PCA work. Just before this discussion the face-feature plot is shown, and while the lookalikes are distributed over the whole PCA space, it is a normal behaviour since we are just using 2 principal components and we of course expects lookalikes to have similar features. The other classes (Jesse and Mila), on the other hand, are clearly separated between them, even with only two principal components.","metadata":{}},{"cell_type":"markdown","source":"# 2. Evaluation Metrics\n","metadata":{"papermill":{"duration":0.10088,"end_time":"2021-03-08T07:59:07.406787","exception":false,"start_time":"2021-03-08T07:59:07.305907","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"The evaluation metrics used throughout the assignment are measures that give us the performance of the face recognition system as a whole. While there are other ones that provide us with the similarity or distance between predicted and true identities that might be used internally in different implemented methods or packages, we decided to just focus on those first ones. Below, we discuss these measures in the context of a binary classification problem for simplicity, with the two classes called positives and negatives. Then, we move to the 3-label classification case that is present in this assignment.","metadata":{}},{"cell_type":"markdown","source":"## 2.2 Accuracy","metadata":{}},{"cell_type":"markdown","source":"The first one, which was already provided in the template, was **accuracy**. Accuracy gives us the ratio between the number of correct predictions and the total amount of predictions:\n\\begin{equation*}\n    \\text{accuracy}  = \\frac{\\text{TP} +\\text{TN} }{\\text{TP} +\\text{TN} +\\text{FP}+\\text{FN} }\n\\end{equation*}\nwhere TP stands for *true positives*, TN for *true negatives*, FP for *false positives* and FN for *false negatives*. However, since we have a multilabel classification problem, the resulting score means the same but the formula gets slightly modified:\n\\begin{equation*}\n    \\text{accuracy}  = \\frac{\\text{number of correctly predicted labels} }{\\text{total number of predictions} }\n\\end{equation*}\n\nAlthough it is used a lot in classification, and it will also be used here, it has a couple of clear *disadvantages* that force us to use other measure as well. Its clearest flaw is its misleadingness when the data is imbalanced. When a class has many more examples than others, a classifier that always predicts the majority class would get a really high accuracy while in reality it would be quite poor if it cannot properly label any other class. However, accuracy it is still a decent starting point to measure the performance of a classifier. In our case, our class distributions are indeed not evenly balanced, but the degree of imbalance is relatively moderate and accuracy gives a rough estimate of a classifier's performance.\n\n","metadata":{}},{"cell_type":"markdown","source":"## 2.3 Precision","metadata":{}},{"cell_type":"markdown","source":"Precision is the next score, which measures the model's ability to correctly identify positive examples. For a binary classification problem, the formula is:\n\\begin{equation*}\n    \\text{precision}  = \\frac{\\text{TP} }{\\text{TP} +\\text{FP} }\n\\end{equation*}\nwhere TP stands for *true positives* and FP for *false positives*.\n\nFor the 3-label classification scenario, each label will get the precision score calculated separately, using the same formula as before. The only change would be that here, TP would be the number of examples predicted correctly as being a specific label *(0, 1 or 2)*, and FP the number of them predicted as belonging to that same label, but actually being from a different one.\n\nPrecision is easy to interpret and useful in multi-class cases, since it can show in our case which is the label with a higher number of false positives. However, it also has its couple of main drawbacks. The first one would be the lack of accountability for false negatives, which might be another useful number to have in mind; and a sensitivity towards class imbalance, in a similar way as in accuracy. That is why it should and it is actually used in conjunction with the following two metrics, which complement precision perfectly to *dodge* its flaws.","metadata":{}},{"cell_type":"markdown","source":"## 2.4 Recall","metadata":{}},{"cell_type":"markdown","source":"**Recall** tells us the performance of the system as the ratio between the true positives to the sum of those same ones and the false positives. It basically measures the proportion of positive examples that are predicted correctly by the system. Its formula is similar to the previous one with a slight change in the denominator to consider *false negatives* instead of *false positives*:\n\\begin{equation*}\n    \\text{recall}  = \\frac{\\text{TP} }{\\text{TP} +\\text{FN} }\n\\end{equation*}","metadata":{}},{"cell_type":"markdown","source":"\nFor the 3-label classification case, the behaviour is exactly the same as for precision. Each label gets its recall score calculated separately with the formula above. Although in this measure, its peak utility is shown in situations where identifying all instances of a class is important and false negatives have notable consequences. A high score would indicate that the system is able to identify correctly most of the relevant instance of a class. However, it has similar flaws as its predecessor, and that is why both of them get calculated together for accountability for false positives and false negatives.","metadata":{}},{"cell_type":"markdown","source":"## 2.4 F1-score","metadata":{}},{"cell_type":"markdown","source":"The F1-score is the harmonic mean of precision and recall, which provides a balance between these two, and therefore, it gives us a better overview of the performance of the system together with the previously mentioned recall. It is calculated as follows:\n\\begin{equation*}\n    f1 = \\frac{2 \\cdot \\text{precision} \\cdot \\text{recall} }{\\text{precision} +\\text{recall}} = \\frac{2  \\text{TP}}{2  \\text{TP}+\\text{FP}+\\text{FN}}\n\\end{equation*}","metadata":{}},{"cell_type":"markdown","source":"The score ranges between 0 and 1, as for all the previous ones as well, even though the meaning of having a score of *1* would be that a perfect precision and recall have been achieved. When there are no specific preferences towards better precision or better recall, the F1-score can be used to easily compare different models. The one that has the higher F1-score being the better-performing one. Therefore, it is really useful in our case when trying to come up with different classifiers and different ways of getting feature representations, and it is the main metric mentioned in the discussions about the results of the classifiers.\n\nExactly as in the previous metrics, for the 3-label classification case each label gets its F1-score independently calculated. However, after obtaining the F1-scores for all the labels, we also calculate what it is called the *macro average F1-score* and *micro average F1-score*. This first new measure is the unweighted average of the F1-scores for each class, which gives equal importance to each one. On the other hand, the *micro average F1-score* gets calculated using a single precision and recall value for the entirety of the data.\n\nEither way, all of these measures derived from the original, binary F1-score are a good metric that avoid the main flaws of recall and precision and provide a good, intuitive metric for checking the overall performance of a system.","metadata":{}},{"cell_type":"markdown","source":"## 2.5 Confusion matrix","metadata":{}},{"cell_type":"markdown","source":"While the previous discussion relied on a binary classification problem as a starting point, since our face recognition application is actually a multi-class classification problem where there are three possible labels: *1* for *Jesse*, *2* for *Mila* and *0* for their lookalikes; in this subsection we will move directly to the multiclass scenario. So, in this kind of problems, confusion matrices are convenient to estimate the performance of a classifier. A confusion matrix is a table where the differences between the true labels and the predicted ones for each class can be seen. Therefore, it is one of the most useful metrics/tools can we can use. Its structure would be like this:\n<table>\n  <tr>\n    <th></th>\n    <th>Actual Label 1</th>\n    <th>Actual Label 2</th>\n    <th>Actual Label 3</th>\n  </tr>\n  <tr>\n    <td><strong>Predicted Label 1</strong></td>\n    <td>TP for Label 1</td>\n    <td>FP for Label 1</td>\n    <td>FP for Label 1</td>\n  </tr>\n  <tr>\n    <td><strong>Predicted Label 2</strong></td>\n    <td>FN for Label 2</td>\n    <td>TP for Label 2</td>\n    <td>FP for Label 2</td>\n  </tr>\n  <tr>\n    <td><strong>Predicted Label 3</strong></td>\n    <td>FN for Label 3</td>\n    <td>FN for Label 3</td>\n    <td>TP for Label 3</td>\n  </tr>\n</table>\n","metadata":{}},{"cell_type":"markdown","source":"This matrix is where the previous metrics get calculated from, but it is also a great tool of evaluating the different models on its own, since it gives a complete breakdown that leads to knowing on which classes the system is performing well and which ones need some additional work. You can easily tell for each case what the main problems and strengths might be if you are getting low or high scores in any of the previous measures.","metadata":{}},{"cell_type":"markdown","source":"# 3. Classifiers\n\nIn the previous section, we extracted features out of the image data of the faces of the training set. These features can then serve as input for various classification methods, which then try to learn to identify the different classes (recognize the different faces) based solely on their feature representations. In this section, we will explore classifying our faces with our handcrafted SIFT and PCA features with various classification methods. ","metadata":{"papermill":{"duration":0.103749,"end_time":"2021-03-08T07:59:08.894358","exception":false,"start_time":"2021-03-08T07:59:08.790609","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"There are many possible classifiers that can be used for our classification problem. Below, we use a support vector machine (SVM) classifier for the SIFT and PCA features that we extracted above. For another tutorial on how to use an SVM for face recognition, see [here](https://scikit-learn.org/stable/auto_examples/applications/plot_face_recognition.html). We also briefly explore a random forest classifier as well as ensemble methods to try to improve these classification methods. Afterwards, we also briefly explore a fully-connected neural network as alternative to the SVM. Finally, we will explore VGG-16 as a classifier, which achieves a superior performance due to the fact that this architecture was trained on much more data, and moreover, uses features obtained using deep learning on a larger training set. \n\nFirst, we define a general pipeline that can be used by any feature extractor and any classifier trained on those features. The function takes a preprocessed image as an input (for the test data, we preprocessed the data before and saved a path to the location of all the preprocessed data), as well as a feature extractor and a classifier.","metadata":{}},{"cell_type":"code","source":"def prediction_pipeline(feature_extractor, classifier, path=test_prep_X_loc):\n    \"\"\"\n    Simple prediction pipeline in case we load in the preprocessed test data.\n    Args:\n        feature_extractor (Any): The feature extractor used to extract features out of faces.\n        classifier (Any): The classifier trained to classify based on the features extracted by feature_extractor.\n        path (str, optional): Location of the directory containing the preprocessed test data to load and classify.\n    \"\"\"\n    # Path points to a directory where preprocessed test data is located\n    y_pred = []\n    \n    # Get the number of files\n    nb_of_files = len(os.listdir(path))\n    for i in tqdm(range(nb_of_files)):\n        # Load data\n        img = np.load(os.path.join(path, f\"test_{i}.npy\"))\n        # Convert from BGR to RGB (warning! cv2.cvtColor gives error)\n        img = img[...,::-1]\n        # Get the features using the extractor\n        features = feature_extractor([img])\n        # Make the predictions\n        new_pred = classifier(features)\n        y_pred.append(new_pred)\n        \n    return y_pred","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:58:05.131986Z","iopub.status.busy":"2023-04-12T14:58:05.131391Z","iopub.status.idle":"2023-04-12T14:58:05.138799Z","shell.execute_reply":"2023-04-12T14:58:05.137767Z","shell.execute_reply.started":"2023-04-12T14:58:05.131941Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We also define an auxiliary function that takes care of converting the predictions we made above to a CSV file for submitting to the Kaggle competition.","metadata":{}},{"cell_type":"code","source":"def predictions_to_csv(predictions: np.array, name: str  = 'submission.csv'):\n    \"\"\"\n    Auxiliary function that creates a CSV submission file based on predictions made for the test set.\n    Args:\n        predictions (np.array): Predictions made on the test set.\n        name (str, optional): Name of the submission CSV file. Defaults to submission.csv\n    \"\"\"\n    submission = test.copy().drop('img', axis = 1)\n    submission['class'] = predictions\n    submission.to_csv(name)\n    \n    return ","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:28:45.568874Z","iopub.status.busy":"2023-04-12T14:28:45.567441Z","iopub.status.idle":"2023-04-12T14:28:45.574839Z","shell.execute_reply":"2023-04-12T14:28:45.573782Z","shell.execute_reply.started":"2023-04-12T14:28:45.568825Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.1 SVM Classifier based on PCA Features","metadata":{}},{"cell_type":"markdown","source":"We begin with an SVM classifier based on the PCA features discussed above. An SVM, or **Support Vector Machine**, is a classifier that maximizes the distance between two classes when they are linearly separable. When data is not linearly separable, a kernel function is applied to transform the data into a higher-dimensional space such a way that a linear decision boundary can still be found within this other space. This is often referred to as the kernel trick. Our aim is that with the SIFT and PCA features, we can find a linear separation between the features of Jesse Eisenberg and Mila Kunis. For that purpose, we use a radial basis function (RBF) kernel (also known as Gaussian kernel) below, since this is the most widely applicable kernel function.\n\nFor a more general info on the use of SVMs in the context of face recognition, see [this tutorial](https://scikit-learn.org/stable/auto_examples/applications/plot_face_recognition.html).","metadata":{}},{"cell_type":"code","source":"class SVM_PCA_classifier:\n    \"\"\"\n    Implements an SVM classifier using the PCA features.\n    \"\"\"\n\n    def __init__(self):\n        # The model will be fitted layer on\n        self.model = None\n\n\n    def fit(self, X: np.array, y: np.array, verbose: bool = True):\n        \"\"\"\n        Fit the SVM with the given data.\n        Args:\n            X (np.array): Input data for the classifier.\n            y (np.array): Output data of the classifier.\n            verbose (bool, optional): Show optimal SVM parameters. Defaults to False.\n        \"\"\"\n\n        # Find the best SVM parameters for a \n        param_grid = {\n            \"C\": loguniform(1e-1, 1e5),\n            \"gamma\": loguniform(1e-7, 1e2),\n        }\n        clf = RandomizedSearchCV(\n            SVC(kernel=\"rbf\", class_weight=\"balanced\"), param_grid, n_iter=500\n        )\n        clf = clf.fit(X, y)\n        if verbose:\n            print(clf.best_estimator_)\n\n        # Save the best model\n        self.model = clf.best_estimator_\n\n    def predict(self, X):\n        return self.model.predict(X)\n\n    def __call__(self, X):\n        return self.predict(X)","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:28:53.686402Z","iopub.status.busy":"2023-04-12T14:28:53.685453Z","iopub.status.idle":"2023-04-12T14:28:53.695097Z","shell.execute_reply":"2023-04-12T14:28:53.693937Z","shell.execute_reply.started":"2023-04-12T14:28:53.686362Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below, we demonstrate the workings of the classifier. First, we transform the input data to obtain their features, after which we perform a train-test-split to have a validation set to check the results of the classifier.","metadata":{}},{"cell_type":"code","source":"# Fit a PCA extractor\nn_components = 40\npca = PCAFeatureExtractor(n_components = n_components, use_color=True)\npca.fit(train_X)\n# Obtain feature representations\ntrain_X_pca = pca.transform(train_X)\n# Perform train test split\nX_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(\n    train_X_pca, train_y, test_size=0.25, random_state=42\n)","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:28:55.145131Z","iopub.status.busy":"2023-04-12T14:28:55.144454Z","iopub.status.idle":"2023-04-12T14:28:58.346238Z","shell.execute_reply":"2023-04-12T14:28:58.344672Z","shell.execute_reply.started":"2023-04-12T14:28:55.145091Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we will use the above training data to fit the SVM classifier.","metadata":{}},{"cell_type":"code","source":"svm_pca_classifier = SVM_PCA_classifier()\nsvm_pca_classifier.fit(X_train_pca, y_train_pca)","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:29:01.897549Z","iopub.status.busy":"2023-04-12T14:29:01.897174Z","iopub.status.idle":"2023-04-12T14:29:06.357335Z","shell.execute_reply":"2023-04-12T14:29:06.355757Z","shell.execute_reply.started":"2023-04-12T14:29:01.897515Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After training, we can check the results by looking at the performance on the hold-out set. Below we see the results on the PCA features in the form of a **confusion matrix**. In all of our results we note the confusion matrix rather than just the accuracy as in a classification task, accuracy can be misleading for imbalanced classes. Rather we look at the f1-score, which is the average of the precision and recall. In our case, the training data that we will be using our classification methods on have balanced classes, but the confusion matrix allows us to see the classification f1-score on each class individually. This will allow us to determine if our features or our model are more distinguishable on a specific class.\n\nBelow we see that the SVM Classifier on the PCA features have a fairly evenly distributed f1-score between all 3 classes. The average f1-score on this classifier is .82","metadata":{}},{"cell_type":"code","source":"# Make the predictions on the test set (validation set)\npred_y = svm_pca_classifier.predict(X_test_pca)\n\n# Print a classification report and show confusion matrix\ncr = classification_report(y_test_pca, pred_y)\nprint(cr)\nif SHOW_PLOTS:\n    ConfusionMatrixDisplay.from_predictions(y_test_pca, pred_y, xticks_rotation=\"vertical\")\n    # Add title and tidy up\n    classifier_name = \"SVM with PCA\"\n    plt.title(\"Confusion matrix for \" + classifier_name)\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:29:18.061547Z","iopub.status.busy":"2023-04-12T14:29:18.061191Z","iopub.status.idle":"2023-04-12T14:29:18.370346Z","shell.execute_reply":"2023-04-12T14:29:18.369380Z","shell.execute_reply.started":"2023-04-12T14:29:18.061513Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2 SVM Classifier based on SIFT Features\n\nSimilarly, we use an SVM Classifier on the handcrafted SIFT Features.\n\nIn order to perform classification on the SIFT Features, we must first create a 'bag of visual words' model. In this scenario, we use K-means clustering to form clusters of features that are distinguishable from each other. The visual features are analogous to words as with this model we create a dictionary where we assign one word to each cluster. Thus each distinguishable feature should be mapped to a specific cluster which will ideally allow the SVM model to group clusters together according to who they identify.","metadata":{}},{"cell_type":"code","source":"class SVM_SIFT_Classifier:\n    \"\"\"\n    SVM classifier using the SIFT features.\n    \"\"\"\n\n    def __init__(self, k: int):\n        \"\"\"\n        Initialization.\n        Args:\n            k (int): Hyperparameter for mini batch k means.\n        \"\"\"\n        self.k = k\n        self.hist = None\n        self.clp = None\n        self.kmeans = None\n\n    def flatten(self, arr):\n        \"\"\"\n        Flatten descriptors to two dimensions\n        Args:\n            arr (np.array): The SIFT descriptors\n        \"\"\"\n        all_descriptors = []\n        for img_descriptors in arr:\n            # get each feature from image\n            for descriptor in img_descriptors:\n                all_descriptors.append(descriptor)\n        all_descriptors = np.stack(all_descriptors)\n        return all_descriptors\n\n    def fit(self, desc):\n        \"\"\"\n        Fit data with mini batch k means\n        Args:\n            desc (np.array): SIFT descriptors\n        \"\"\"\n        flattened_desc = self.flatten(desc)\n        kmeans = MiniBatchKMeans(n_clusters=self.k, batch_size=20).fit(flattened_desc)\n        self.kmeans = kmeans\n\n    def histogram(self, all_kp, all_desc):\n        \"\"\"\n        Cluster data into histograms based on prediction.\n        Args:\n            all_kp (np.array): All keypoints for SIFT\n            all_desc (np.array): All descriptors for SIFT\n        \"\"\"\n        hist = []\n        for i in range(len(all_desc)):\n            hist_i = np.zeros(self.k)\n            nkp = np.size(all_kp[i])\n\n            for d in all_desc[i]:\n                idx = self.kmeans.predict([d])\n                hist_i[idx] +=1/nkp\n            hist.append(hist_i)\n        self.hist = hist\n        return hist\n\n    def train(self, train_hist, train_y, verbose = True):\n        \"\"\"\n        Train the classifier\n        Args:\n            train_hist (np.array): Histogram\n            train_y (np.array): Targets\n            verbose (bool, optional): Print progress. Defaults to False.\n        \"\"\"\n        # Perform grid search to find optimal SVM parameters\n        param_grid = {\n            \"C\": loguniform(1e-1, 1e5),\n            \"gamma\": loguniform(1e-7, 1e2),\n        }\n        clf = RandomizedSearchCV(\n            SVC(kernel=\"rbf\", class_weight=\"balanced\"), param_grid, n_iter=500)\n        start = time.time()\n        clf = clf.fit(np.array(train_hist), train_y)\n        end = time.time()\n        if verbose:\n            print(\"Done in %0.3fs\" % (end - start))\n            print(\"Best estimator found by grid search:\")\n            print(clf.best_estimator_)\n        # Save best classifier\n        self.clf = clf\n        return clf","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:29:26.291784Z","iopub.status.busy":"2023-04-12T14:29:26.291259Z","iopub.status.idle":"2023-04-12T14:29:26.303361Z","shell.execute_reply":"2023-04-12T14:29:26.302260Z","shell.execute_reply.started":"2023-04-12T14:29:26.291744Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As before, we first collect the SIFT Features for our classification problem and perform a train-test-split.","metadata":{}},{"cell_type":"code","source":"# Create instance of the classifier\nclassifier = SVM_SIFT_Classifier(10)\n\n# split training data\nX_train_sift, X_test_sift, y_train_sift, y_test_sift = train_test_split(\n    train_X, train_y, test_size=0.25, random_state=42\n)\n\n# get descriptors and keypoints of train data\ntrain_des, train_kp = SIFTExtractor.detect_and_compute(X_train_sift)\nclassifier.fit(train_des)\n# cluster training points by their keypoints\ntrain_hist = classifier.histogram(train_kp, train_des)\n# train\nclf = classifier.train(train_hist, y_train_sift)\n\n# get desscriptors and keypoints of test data\ntest_des, test_kp = SIFTExtractor.detect_and_compute(X_test_sift)\n# cluster test data by keypoints\ntest_hist = classifier.histogram(test_kp, test_des)","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:29:31.442920Z","iopub.status.busy":"2023-04-12T14:29:31.442174Z","iopub.status.idle":"2023-04-12T14:29:36.953001Z","shell.execute_reply":"2023-04-12T14:29:36.951780Z","shell.execute_reply.started":"2023-04-12T14:29:31.442881Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We now again turn to the prediction stage. We note that the SIFT Features result in lower performance with SVM than the PCA Features. The average F1-score is .5\n\nThis lower score can either be attributed to the fact that an SVM classsifier may not be the best model for these features, or that the SIFT Features are less distinguishable than the PCA Features.","metadata":{}},{"cell_type":"code","source":"# make predictions\npred_y = classifier.clf.predict(test_hist)\n\n# Show report\ncr = classification_report(y_test_sift, pred_y)\nprint(cr)\nif SHOW_PLOTS:\n    ConfusionMatrixDisplay.from_predictions(y_test_sift, pred_y, xticks_rotation=\"vertical\")\n    # Add title and tidy up\n    classifier_name = \"SVM with SIFT\"\n    plt.title(\"Confusion matrix for \" + classifier_name)\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:29:39.538831Z","iopub.status.busy":"2023-04-12T14:29:39.538070Z","iopub.status.idle":"2023-04-12T14:29:39.809503Z","shell.execute_reply":"2023-04-12T14:29:39.808513Z","shell.execute_reply.started":"2023-04-12T14:29:39.538785Z"},"papermill":{"duration":0.108542,"end_time":"2021-03-08T07:59:09.525054","exception":false,"start_time":"2021-03-08T07:59:09.416512","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.3 RandomForest Classifer\n\nNext we try a random forest classifier from sklearn. SVM Classifiers are more performant on binary classification tasks. Since our problem is a multi-class classficication problem, we expect the RandomForest Classifier to perform better.\n\nA **random forest classifier** is an ensemble method of classification such that it combines predictions from multiple decision trees. Each decision tree uses a random subset of the training data and features, hence the name 'random' forest. In this way, the random forest classifier should account for overfitting and lead to better performance especially on small datasets where overfitting is common.\n\nWe tune the `max_depth` parameter in order to obtain the best results for our data. Selecting a large number can lead to overfitting, while a small number can lead to underfitting.\n\nAfterwards, we use the random forest classifier for both the SIFT and PCA feature representations of the data.","metadata":{}},{"cell_type":"code","source":"class BestModel:\n\n    def __init__(self):\n        self.model = None\n\n    def fit(self, X, y):\n        self.model = RandomForestClassifier(max_depth=6, random_state=1)\n        self.model.fit(X, y)\n\n\n    def predict(self, X):\n        return self.model.predict(X)","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:29:41.522127Z","iopub.status.busy":"2023-04-12T14:29:41.521457Z","iopub.status.idle":"2023-04-12T14:29:41.528356Z","shell.execute_reply":"2023-04-12T14:29:41.527104Z","shell.execute_reply.started":"2023-04-12T14:29:41.522087Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.3.1 RandomForestClassifier based on SIFT Features\n\nWe start by modeling the classier on the SIFT Features","metadata":{}},{"cell_type":"code","source":"sift_search_classifier = BestModel()\nsift_search_classifier.fit(np.array(train_hist), y_train_sift)","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:30:34.869952Z","iopub.status.busy":"2023-04-12T14:30:34.869557Z","iopub.status.idle":"2023-04-12T14:30:34.991860Z","shell.execute_reply":"2023-04-12T14:30:34.990880Z","shell.execute_reply.started":"2023-04-12T14:30:34.869919Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the predictions that follow, we note that the average f1-score did not improve but the metrics for Class 1 and Class 2 improved. These are the classes for Jesse Eisenberg and Mila Kunis which are the two classes that we wish to most accurately discriminate. However the lower score on Class 0 means that the random forest classsifier classifies 'lookalikes' less accurately.","metadata":{}},{"cell_type":"code","source":"pred_y = sift_search_classifier.predict(test_hist)\n\n# Show report\ncr = classification_report(y_test_sift, pred_y)\nprint(cr)\nif SHOW_PLOTS:\n    ConfusionMatrixDisplay.from_predictions(y_test_sift, pred_y, xticks_rotation=\"vertical\")\n    # Add title and tidy up\n    classifier_name = \"random forest with SIFT\"\n    plt.title(\"Confusion matrix for \" + classifier_name)\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:30:45.716629Z","iopub.status.busy":"2023-04-12T14:30:45.715965Z","iopub.status.idle":"2023-04-12T14:30:46.092834Z","shell.execute_reply":"2023-04-12T14:30:46.091349Z","shell.execute_reply.started":"2023-04-12T14:30:45.716568Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.3.2 Random Forest Classifer based on PCA features\n\nNext we use the classifier on the PCA features","metadata":{}},{"cell_type":"code","source":"pca_classifier = BestModel()\naml = pca_classifier.fit(X_train_pca, y_train_pca)","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:30:59.197916Z","iopub.status.busy":"2023-04-12T14:30:59.197546Z","iopub.status.idle":"2023-04-12T14:30:59.321972Z","shell.execute_reply":"2023-04-12T14:30:59.320975Z","shell.execute_reply.started":"2023-04-12T14:30:59.197883Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Compared to SVM, our RandomForest classifer performs slighlty worse. Additionally, with the random forest classifier we still observe that the PCA classifier performs better than the SIFT classifier. Thus it can be concluded that the PCA features are more discriminative than the SIFT features.","metadata":{}},{"cell_type":"code","source":"pred_y = pca_classifier.model.predict(X_test_pca)\n\n# Show report\ncr = classification_report(y_test_pca, pred_y)\nprint(cr)\nif SHOW_PLOTS:\n    ConfusionMatrixDisplay.from_predictions(y_test_pca, pred_y, xticks_rotation=\"vertical\")\n    # Add title and tidy up\n    classifier_name = \"random forest with PCA\"\n    plt.title(\"Confusion matrix for \" + classifier_name)\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:31:09.920406Z","iopub.status.busy":"2023-04-12T14:31:09.919449Z","iopub.status.idle":"2023-04-12T14:31:10.274600Z","shell.execute_reply":"2023-04-12T14:31:10.273656Z","shell.execute_reply.started":"2023-04-12T14:31:09.920367Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since our training dataset is so small with only 85 examples, we believe our classifier may be dropping in performance due to overfitting. Another method to prevent overfitting is cross validation. Cross validation splits the datatset into partitions and performs a train_test split on each partition and takes an average of all of the results. We note that cross validation improves the performance of the RandomForest Classifier on PCA features. The average f1-score seen below is .85 Cross validation was also experiemnted on the SIFT features, however it did not affect the scores so it is not shown in this demonstration.","metadata":{}},{"cell_type":"code","source":"y_pred = cross_val_predict(pca_classifier.model, train_X_pca, train_y, cv=3)\ncr = classification_report(train_y, y_pred)\nprint(cr)\nif SHOW_PLOTS:\n    ConfusionMatrixDisplay.from_predictions(train_y, y_pred, xticks_rotation=\"vertical\")\n    # Add title and tidy up\n    classifier_name = \"random forest with PCA, cross-validation\"\n    plt.title(\"Confusion matrix for \" + classifier_name)\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:33:42.936270Z","iopub.status.busy":"2023-04-12T14:33:42.935809Z","iopub.status.idle":"2023-04-12T14:33:43.576559Z","shell.execute_reply":"2023-04-12T14:33:43.575552Z","shell.execute_reply.started":"2023-04-12T14:33:42.936230Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.4 Ensemble Classification\n\nA final method to improve on our SVM classification methods discussed above is to combine the predictions from both the SIFT classifier and the PCA classifier in a so-called **ensemble method**. The SIFT and PCA features may highlight different identifiers for each person. Thus creating this ensemble classifier allows us to combine both feature types to improve on our classification. Our ensemble classifier results in an f1-score of .73\n\nFrom the results that follow, we see that our ensemble classifier performs the best on class 2. This means that the PCA and SIFT classifiers agreed on the most images of Mila Kunis. While the ensemble classifier performs the worst on class 0.","metadata":{}},{"cell_type":"code","source":"pred_proba_sift = sift_search_classifier.model.predict_proba(np.array(test_hist))\npred_proba_pca = pca_classifier.model.predict_proba(X_test_pca)\n\ncombined_pred_proba = (pred_proba_pca + pred_proba_sift) /2\ncombined_pred = np.argmax(combined_pred_proba, axis=1)\n\ncr = classification_report(y_test_pca, combined_pred)\nprint(cr)\nif SHOW_PLOTS:\n    ConfusionMatrixDisplay.from_predictions(y_test_pca, combined_pred, xticks_rotation=\"vertical\")\n    # Add title and tidy up\n    classifier_name = \"ensemble method\"\n    plt.title(\"Confusion matrix for \" + classifier_name)\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:33:54.278905Z","iopub.status.busy":"2023-04-12T14:33:54.277975Z","iopub.status.idle":"2023-04-12T14:33:54.569639Z","shell.execute_reply":"2023-04-12T14:33:54.568530Z","shell.execute_reply.started":"2023-04-12T14:33:54.278852Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.5 Simple feedforward MLP","metadata":{}},{"cell_type":"markdown","source":"Before we turn our attention to the final and most effective classifier, the VGG network, we discuss a simple neural network architecture as build-up. That is, we use a fully-connected (dense) **feedforward multilayer perceptron** (MLP), which takes our handcrafted features as input features, and propagates them through a few hidden layers to compute an output. In our case, the output will consist of three nodes that represent the probabilities that a given feature representation corresponds to the given classes 0, 1, 2. We will demonstrate the inner workings of neural networks using the PyTorch library. For our purpose, one can choose whether to convert the targets, *i.e.* the class labels, to 3D vectors for this purpose using the auxiliary function defined below, although PyTorch is able to handle both representations.","metadata":{}},{"cell_type":"code","source":"def create_vector_data(labels: np.array, size: int=3):\n    \"\"\"\n    Turns an array of class labels into an array of 3D vectors representing probabilities.\n    Args:\n        labels (np.array): Class labels (target data)\n        size (int, optional): Number of different class labels. Defaults to 3 for our application.\n    Returns:\n        np.array: New labels, representing the distributions.\n    \"\"\"\n    new_labels = []\n    for label in labels:\n        # Put zeroes everywhere\n        new_label = np.zeros(size)\n        # Put a one in the index location corresponding to the label\n        new_label[label] = 1\n        new_labels.append(new_label)\n        \n    return np.array(new_labels)","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:33:59.366128Z","iopub.status.busy":"2023-04-12T14:33:59.365392Z","iopub.status.idle":"2023-04-12T14:33:59.372110Z","shell.execute_reply":"2023-04-12T14:33:59.371056Z","shell.execute_reply.started":"2023-04-12T14:33:59.366089Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The PyTorch library allows user to create their own datasets to process data in PyTorch networks. Hence, we define a custom dataset class that is specifically catered towards our face recognition application. Note that, when loading in the data, we normalize the input data, which is good practice especially when dealing with high-dimensional data.","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    \"\"\"\n    Custom data set to process the input features and output labels of our data.\n    \"\"\"\n\n    def __init__(self, features: np.array, labels: np.array, to_vector: bool = False, normalization_function: Callable = None) -> None:\n        \"\"\"\n        Initializes the class by separating data into features and labels.\n        Args:\n            features (np.array): Array containing the input data.\n            labels (np.array): Array containing the output data.\n            to_vector (bool, optional): Indicate whether we train with class labels as targets or convert to probabilities. Defaults to False.\n            normalization_function (Callable, optional): Function that normalizes the input features, for instance with scikit-learn. Defaults to None (no normalization performed).\n        \"\"\"\n        # Normalize input data if given a normalization function to perform\n        if normalization_function is not None:\n            features = normalization_function(features)\n\n        # Convert features to Torch tensors for PyTorch\n        features = torch.from_numpy(features)\n        # Turn into probabilities if desired (not recommended according to PyTorch docs)\n        if to_vector:\n            labels = create_vector_data(labels)\n        # Convert labels to Torch tensors\n        labels = torch.from_numpy(labels)\n\n        # Save as instance variables to the dataloader\n        self.features = features\n        self.labels   = labels\n\n    def __len__(self):\n        \"\"\"Get the length of the dataset.\"\"\"\n        return len(self.labels)\n\n    def __getitem__(self, idx: int):\n        \"\"\"\n        Gets an item from the dataset based on a specified index.\n        Args:\n            idx: (int): Index used to fetch the item.\n        Returns:\n            tuple[torch.Tensor, torch.Tensor]: Tuple of feature and its corresponding label.\n        \"\"\"\n        # Get the feature, but normalized\n        feature = self.features[idx]\n\n        # Get the label\n        label = self.labels[idx]\n        return feature, label","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:34:26.756412Z","iopub.status.busy":"2023-04-12T14:34:26.756049Z","iopub.status.idle":"2023-04-12T14:34:26.765840Z","shell.execute_reply":"2023-04-12T14:34:26.764541Z","shell.execute_reply.started":"2023-04-12T14:34:26.756378Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For this demonstration, we will use the features extracted by a PCA analysis (see above for the extraction and train test split), as they are seemingly better features to use compared to the SIFT features, based on our personal observations and experiments. As already mentioned, we will normalize the data through scikit-learn's StandardScaler. We fit the scaler on the training data and use the same transformation to scale the test data. Normalizing the input features is crucial to ensure that the network trains efficiently and to improve the stability of the training process. ","metadata":{}},{"cell_type":"code","source":"# Define StandardScaler object\nscaler = StandardScaler()\n# Convert to PyTorch Datasets as we defined them, normalize with same transformation as on training data\ntrain_dataset = CustomDataset(X_train_pca, y_train_pca, normalization_function = scaler.fit_transform) \ntest_dataset  = CustomDataset(X_test_pca, y_test_pca, normalization_function = scaler.transform)\n# Then we create dataloaders\ntrain_dataloader = DataLoader(train_dataset, batch_size=1)\ntest_dataloader  = DataLoader(test_dataset, batch_size=1)","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:34:39.831461Z","iopub.status.busy":"2023-04-12T14:34:39.831054Z","iopub.status.idle":"2023-04-12T14:34:39.849039Z","shell.execute_reply":"2023-04-12T14:34:39.848042Z","shell.execute_reply.started":"2023-04-12T14:34:39.831419Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will now define our architecture. The architecture can easily accomodate for various setups of the hidden layers and their sizes. The user can also specify different activation functions, with the default one being the rectified linear unit (ReLU), which is often used. The output of the network uses a **softmax layer**, which models a multinomial probability distribution and is hence the ideal activation function for a multiclass classification problems. Applying an argmax operation to the output of the network can turn these probabilities into a prediction.","metadata":{}},{"cell_type":"code","source":"class FeedForwardNet(nn.Module):\n    \"\"\"\n    Implements a simple feedforward neural network for our multiclass classification problem of face recognition.\n    \"\"\"\n    def __init__(self, nb_of_inputs: int, nb_of_outputs: int = 3, h: list = [200, 200, 200], activation_function: Callable = torch.nn.ReLU) -> None:\n        \"\"\"\n        Initialize the neural network class.\n        Args:\n            nb_of_inputs (int): Number of input nodes for the network, i.e. the number of features.\n            nb_of_outputs (int, optional): Number of output nodes for the softmax layer. Defaults to 3.\n            h (list, optional): List representing the setup of the hidden layers by specifying the number of hidden neurons per layer. Defaults to [200, 200, 200].\n            activation_function (Callable, optional): Activation function for the hidden layers. Defaults to the ReLU activation function.  \n        \"\"\"\n        \n        self.h = h\n        # Call the super constructor first\n        super(FeedForwardNet, self).__init__()\n        \n        # Add visible layers as well to get all layer sizes\n        self.h_augmented = [nb_of_inputs] + h + [nb_of_outputs]\n\n        # Define the layers:\n        for i in range(len(self.h_augmented)-1):\n            if i == len(self.h_augmented)-2:\n                # For the final output layer, apply softmax\n                setattr(self, f\"linear{i+1}\", nn.Linear(self.h_augmented[i], self.h_augmented[i+1], bias=False))\n                setattr(self, f\"softmax\", nn.Softmax())  \n            else:\n                # For intermediate layers, apply the specified activation function\n                setattr(self, f\"linear{i+1}\", nn.Linear(self.h_augmented[i], self.h_augmented[i+1]))\n                setattr(self, f\"activation{i+1}\", activation_function())\n\n    def forward(self, x):\n        \"\"\"\n        Computes a single forward step given the input x.\n        Args:\n            x (torch.Tensor): Input for the neural network.\n        Returns:\n            torch.Tensor: Output of the network.\n        \"\"\"\n\n        for i, module in enumerate(self.modules()):\n            # The first module is the whole network, so continue\n            if i == 0:\n                continue\n            # For each of our defined layers, \"apply\" the layer\n            x = module(x)\n\n        return x","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:34:42.109397Z","iopub.status.busy":"2023-04-12T14:34:42.109042Z","iopub.status.idle":"2023-04-12T14:34:42.120876Z","shell.execute_reply":"2023-04-12T14:34:42.119882Z","shell.execute_reply.started":"2023-04-12T14:34:42.109364Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that the architecture is defined, we will define a **train loop**, in wich the neural networks' parameters get adjusted, and a **test loop**, where the weights are frozen and we compute the loss on the train and test set to monitor the training.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def train_loop(dataloader: DataLoader, model: FeedForwardNet, loss_fn: Callable, optimizer: Callable) -> None:\n    \"\"\"\n    Does one epoch of the training loop.\n    Args:\n        dataloader (DataLoader): Torch DataLoader object, containing training data.\n        model (FeedForwardNet): An instance of our neural network.\n        loss_fn (Callable): The loss function used during training.\n        optimizer (Callable): The optimizer used during training.\n    \"\"\"\n    size = len(dataloader.dataset)\n    # Go over the data in the dataloader\n    for batch, (X, y) in enumerate(dataloader):\n        # Compute prediction \n        prediction = model(X)\n        # Compute loss \n        loss = loss_fn(prediction, y)\n\n        # Do the backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\ndef test_loop(dataloader: DataLoader, model: FeedForwardNet, loss_fn: Callable) -> float:\n    \"\"\"\n    The testing loop to compute the loss.\n    Args:\n        dataloader (DataLoader): Torch DataLoader object, containing training data.\n        model (FeedForwardNet): An instance of our neural network.\n        loss_fn (Callable): The loss function used during training.\n        optimizer (Callable): The optimizer used during training.\n    Returns:\n        float: Loss computed on the provided data.\n    \"\"\"\n\n    # Get the number of batches\n    num_batches = len(dataloader)\n    # Initialize loss\n    test_loss = 0\n\n    # Predict and compute loss, add to total loss\n    with torch.no_grad():\n        for X, y in dataloader:\n            prediction = model(X)\n            test_loss += loss_fn(prediction, y).item()\n\n    # Return the average of the loss over the number of batches\n    return test_loss / num_batches","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:34:43.673304Z","iopub.status.busy":"2023-04-12T14:34:43.672946Z","iopub.status.idle":"2023-04-12T14:34:43.681770Z","shell.execute_reply":"2023-04-12T14:34:43.680601Z","shell.execute_reply.started":"2023-04-12T14:34:43.673272Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Imbalanced data can disrupt the training and effectiveness of the classifier. While the training data is not too imbalanced, we will take the class distributions into account when training the MLP, since that will be easy to achieve with PyTorch.","metadata":{}},{"cell_type":"code","source":"# Get class distributions of the training data\ncounts = np.array([len(train_improved[train_improved[\"class\"] == i]) for i in [0,1,2]])\nprobabilities = counts/np.sum(counts)\nprobabilities = torch.from_numpy(probabilities)\nprobabilities","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:34:44.936474Z","iopub.status.busy":"2023-04-12T14:34:44.935883Z","iopub.status.idle":"2023-04-12T14:34:45.019366Z","shell.execute_reply":"2023-04-12T14:34:45.018221Z","shell.execute_reply.started":"2023-04-12T14:34:44.936434Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We now implement a class which unites all training aspects and also saves the losses obtained during training to conveniently plot the training afterwards. The train method implemented in this class also uses **early stopping**, a commonly used regularization method, and implements a simple DIY adaptive learning rate scheme.","metadata":{}},{"cell_type":"code","source":"class Trainer:\n    \"\"\"\n    Class that implements all training aspects.\n    \"\"\"\n\n    def __init__(self, model: FeedForwardNet, learning_rate: float, train_dataloader: DataLoader, test_dataloader: DataLoader, \n                 loss_fn: Callable, optimizer: Callable) -> None:\n        \"\"\"\n        Initialization of class.\n        Args:\n            model (FeedForwardNet): An instance of our network.\n            learning_rate (float): Initial learning rate for the network.\n            train_dataloader (DataLoader): DataLoader for the training data.\n            test_dataloader (DataLoader): DataLoader for the test data.\n            loss_fn (Callable): The loss function used during training.\n            optimizer (Callable): The optimizer used during training.\n        \"\"\"\n\n        # Save everything as class variables\n        self.train_dataloader = train_dataloader\n        self.test_dataloader = test_dataloader\n        self.model = model\n        self.best_model = model\n        self.loss_fn = loss_fn\n        self.learning_rate = learning_rate\n        self.optimizer = optimizer\n        # Create empty lists to store the train and test lossees\n        self.train_losses = []\n        self.test_losses = []\n        \n\n    def train(self, number_of_epochs: int = 500, patience: int = 10, patience_delta: float = 0.01, \n              adaptation_threshold: float = 0.9995, adaptation_multiplier: float = 0.5, verbose: bool = False) -> None:\n        \"\"\"\n        Train the network using early stopping and with an adaptive learning rate.\n        Args:\n            number_of_epochs (int, optional): Maximum number of epochs to train the network. Defaults to 500.\n            patience (int, optional): Number of epochs before we exit training due to early stopping criterion. Defaults to 10.\n            patience_delta (float, optional): Threshold when comparing test loss for early stopping criterion. Defaults to 0.01.\n            adaptation_threshold (float, optional): Threshold for the adaptive learning rate. Defaults to 0.9995.\n            adaptation_multiplier (float, optional): Multiplier reducing the learning rate if desired. Defaults to 0.5.\n            verbose (bool, optional): Print epochs and losses during training. Defaults to False.\n        \"\"\"\n        \n        # Initialize variables\n        best_loss = np.inf\n        # Counter for early stopping will have a \"burn in\" period\n        stopping_counter = -10\n\n        # Epoch counter for this specific training session\n        epoch_counter = 0  \n        # In case we continue training, the total number of epochs depends on train losses already saved\n        total_epoch_counter = len(self.train_losses) + 1 \n\n        # The counter makes sure we do not update the learning rate too often\n        # if the network was not trained yet, first 5 epochs we don't change the learning rate\n        if len(self.train_losses) == 0:\n            counter = -5\n        else:\n            counter = 0\n\n        # Keep on continuing the training until we hit max number of epochs\n        while epoch_counter < number_of_epochs:\n            # Train the network\n            train_loop(self.train_dataloader, self.model, self.loss_fn, self.optimizer)\n            # Test on the training data to get performance\n            average_train_loss = test_loop(self.train_dataloader, self.model, self.loss_fn)\n            self.train_losses.append(average_train_loss)\n            # Test on testing data to get performance\n            average_test_loss = test_loop(self.test_dataloader, self.model, self.loss_fn)\n            self.test_losses.append(average_test_loss)\n            \n            # Print progress (if desired)\n            if verbose:\n                print(f\"--- Epoch {epoch_counter} ---\")\n                print(f\"Train loss: {average_train_loss}\")\n                print(f\"Test   loss: {average_test_loss}\")\n\n            ## Adaptive learning rate\n            # Adapt the learning rate after 10 epochs (burn-in period)\n            if counter >= 10:\n                # Compare previous and recent train losses\n                current = np.min(self.train_losses[-5:])\n                previous = np.min(self.train_losses[-10:-5])\n\n                # If we did not improve the test loss sufficiently, adapt learning rate\n                if current / previous >= adaptation_threshold:\n                    # Reset counter (note: we will increment later, so set to -1 st it becomes 0)\n                    counter = -1\n                    self.learning_rate = adaptation_multiplier * self.learning_rate\n                    # Change optimizer\n                    optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n                    self.optimizer = optimizer\n                    \n            ## Early stopping\n            # Check if the validation loss has improved\n            if average_test_loss < best_loss:\n                best_loss = average_test_loss\n                self.best_model = self.model\n                stopping_counter = 0\n            else:\n                # Perform early stopping if there was no improvement after a certain number of timesteps\n                if average_test_loss > best_loss + patience_delta:\n                    stopping_counter += 1\n                    if stopping_counter >= patience:\n                        print('Early stopping after {} epochs'.format(epoch_counter + 1))\n                        break\n\n            # Another epoch passed - increment overall counters\n            counter += 1\n            epoch_counter += 1\n\n        print(\"Training done!\")","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:34:46.331661Z","iopub.status.busy":"2023-04-12T14:34:46.330732Z","iopub.status.idle":"2023-04-12T14:34:46.347317Z","shell.execute_reply":"2023-04-12T14:34:46.346262Z","shell.execute_reply.started":"2023-04-12T14:34:46.331609Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we are ready to train an instance of our MLP classifier. For training, we use stochastic gradient descent (SGD) as optimizer, and the loss function is chosen to be the cross-entropy loss, which is commonly used in multi-class classification problems.","metadata":{}},{"cell_type":"code","source":"# Define an instance of our network, make sure the input matches the number of PCA components\nnet = FeedForwardNet(nb_of_inputs = pca.n_components, h=[100, 20]).double()\nprint(net)\n# Initialize optimizer and Trainer\nlearning_rate = 0.001\nloss_fn = torch.nn.CrossEntropyLoss(weight=probabilities)\noptimizer = torch.optim.SGD(net.parameters(), lr = learning_rate)\ntrainer = Trainer(net, learning_rate, train_dataloader, test_dataloader, loss_fn, optimizer)","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:34:48.569999Z","iopub.status.busy":"2023-04-12T14:34:48.569626Z","iopub.status.idle":"2023-04-12T14:34:48.585381Z","shell.execute_reply":"2023-04-12T14:34:48.583917Z","shell.execute_reply.started":"2023-04-12T14:34:48.569958Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train(number_of_epochs=500)","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:34:51.022034Z","iopub.status.busy":"2023-04-12T14:34:51.021674Z","iopub.status.idle":"2023-04-12T14:35:17.986836Z","shell.execute_reply":"2023-04-12T14:35:17.985649Z","shell.execute_reply.started":"2023-04-12T14:34:51.022001Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Make a plot of the losses to monitor the training process:","metadata":{}},{"cell_type":"code","source":"if SHOW_PLOTS:\n    plt.plot([i+1 for i in range(len(trainer.train_losses))], trainer.train_losses, '-o', color=\"red\", label=\"Train\")\n    plt.plot([i+1 for i in range(len(trainer.test_losses))], trainer.test_losses, '-o', color=\"blue\", label=\"Test\")\n    plt.grid()\n    plt.legend()\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Training of feedforward MLP\")\n    plt.show()","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:35:22.081369Z","iopub.status.busy":"2023-04-12T14:35:22.080672Z","iopub.status.idle":"2023-04-12T14:35:22.304555Z","shell.execute_reply":"2023-04-12T14:35:22.303529Z","shell.execute_reply.started":"2023-04-12T14:35:22.081329Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have a trained model, we can put everything into a classifier class for the prediction pipeline.","metadata":{}},{"cell_type":"code","source":"class FeedForwardClassifier:\n    \"\"\"\n    Implements a classifier with a simple feedforward MLP.\n    \"\"\"\n\n    def __init__(self, net: FeedForwardNet, scaler: StandardScaler):\n        \"\"\"\n        Initializes the classifier with a network and a normalizer.\n        Args:\n            net (FeedForwardNet): A trained feedforward MLP classifier with softmax output layer.\n            scaler (StandardScaler): StandardScaler which normalizes the input data.\n        \"\"\"\n        self.net = net\n        self.scaler = scaler\n\n    def fit(self, X, y):\n        \"\"\"Classifier takes already fitted model\"\"\"\n        pass\n\n    def predict(self, X: np.array) -> np.array:\n        # Disable torch's gradient (backpropagation) for predictions\n        with torch.no_grad():\n            # Perform normalization\n            X = self.scaler.transform(X)\n            # Make the prediction\n            y_pred = self.net(torch.from_numpy(X))\n            y_pred = torch.argmax(y_pred, axis=1).numpy()\n            # In case we have only a single value, convert array to int\n            if len(y_pred) == 1:\n                y_pred = y_pred[0]\n        return y_pred\n\n    def __call__(self, X):\n        return self.predict(X)","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:35:26.126549Z","iopub.status.busy":"2023-04-12T14:35:26.126188Z","iopub.status.idle":"2023-04-12T14:35:26.135208Z","shell.execute_reply":"2023-04-12T14:35:26.133919Z","shell.execute_reply.started":"2023-04-12T14:35:26.126516Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We again analyze the performance with the classification report and confusion matrix. The results are comparable to the previous classifiers. Hence, the MLP can be an alternative to the SVM. Since there are many more adjustible hyperparameters (number if hidden layers, number of hidden neurons, optimizers, loss function, learning rate, momentum, adaptable learning rates,...), the MLP is more flexible in terms of design than the simple SVM classifiers that we discussed before. Hence, future work could try to explore different architecture designs to improve the performance of the MLP classifier.","metadata":{}},{"cell_type":"code","source":"classifier = FeedForwardClassifier(net, scaler)\n# Make the predictions on the test set (validation set)\npred_y = classifier(X_test_pca)\n\n# Print a classification report and show confusion matrix\ncr = classification_report(y_test_pca, pred_y)\nprint(cr)\nif SHOW_PLOTS:\n    ConfusionMatrixDisplay.from_predictions(y_test_pca, pred_y, xticks_rotation=\"vertical\")\n    # Add title and tidy up\n    classifier_name = \"MLP with PCA\"\n    plt.title(\"Confusion matrix for \" + classifier_name)\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.execute_input":"2023-04-12T14:35:28.501891Z","iopub.status.busy":"2023-04-12T14:35:28.500880Z","iopub.status.idle":"2023-04-12T14:35:28.790980Z","shell.execute_reply":"2023-04-12T14:35:28.789841Z","shell.execute_reply.started":"2023-04-12T14:35:28.501840Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.1. Baseline 2: Transfer learning CNN","metadata":{}},{"cell_type":"markdown","source":"We use pre-trained weights and the architecture of a neural network trained on face images called *VGG*. For this, we import a pretrained network from Keras. Then, we append an additional classification layer to the network such that we get a probability distribution using a softmax (as explained earlier on) for the 3 classes of our dataset. The bottom layers in the model serve as feature extractor and then we add the classifier so that everything is integrated into a single model. This has the advantage that with a single architecture we do not have to hand-craft the feature extraction methods, as given enough data the loss function would optimise the weights to pick up the the features that have the best discriminative properties in the given training datasets. **Convolutional layers** are interesting in the model because they reduce the learning burden that would be required if the model instead used dense layers (with the set of associated problems that comes with it, vanishing gradients, hard to conceptually interpret...). Convolutions not only reduce the learning difficulty but allow representing and visualising the learned filters as a convenient construct that has been used in computer vision for decades to process images (extracting edges, blurring, discretising...). Hence, we expect that the features that are extracted through deep learning techniques are much more robust than, say, the features obtained with PCA. Moreover, they take into account local features, and can hence easily deal with different poses, angles,...\n\nInstead of using a classifier layer like the softmax, we could also decide to directly use the *embeddings* from the top layer of the VGG network to calculate face similarities for the reference actors and get the closest ones using pairwise distance functions (such as Euclidean distance). Or one can feed those embeddings as new feature representations to classifiers such as the ones we defined and showcased above. Instead, we preferred to fine-tune the network and therefore perform **transfer-learning** based on our training dataset. This approach has the downside that we may be overfitting on the training and validation datasets, whereas with the embedding we are trusting the pre-trained model to be the most optimal, which is likely the case since these architectures are considered to be state-of-the-art in the field of image recognition. To reduce overfitting we should be freezing some layers or reducing the number of parameters as well as maybe adding dropout layers. This is a matter of experimenting with the intuition behind these choices and is beyond the scope of this tutorial.\n\n\nWe tried multiple VGGFace libraries. In the end, we used the model and weights from DeepFace as we had already imported this library for the face detector. While likely not the best model to exist for our purposes, it is simpler and easier to understand (as it is closer to generic artificial neural networks without domain-specific knowledge) than models that are using custom loss functions with more advanced heuristics (such as siamese networks with contrastive loss, triplet loss, ArcFace...). Image recognition challenges focusing purely on deep learning will surely make use of them!","metadata":{}},{"cell_type":"code","source":"class VGGFaceFeatureExtractorAndClassifier:\n    \"\"\"\n    Implements a feature extractor and classifier based on VGG\n    \"\"\"\n\n    def __init__(self):\n        ## Load (other option for vgg)\n        # vgg = VGG16(include_top=False, input_shape=FACE_SIZE + (3,))\n        \n        ## Default: Load the same pretrained model we are using in previous steps of the pipeline\n        # so we do not have to re-download the weights\n        vgg = DeepFace.VGGFace.loadModel()\n\n        # Beware: a higher number of neurons would overfit on the training dataset\n        classifier = Dense(32, activation='relu')(vgg.layers[-1].output)\n        \n        # Modify top layers to our classification problem\n        output_prob = Dense(3, activation='softmax')(classifier)\n\n        # Save modified model with the new layers\n        self.model = Model(inputs=vgg.inputs, outputs=output_prob)\n\n    def fit(self, X, y):\n        # Use the same preprocessing as the pretrained model\n        X_train = vgg16_preprocess_input(X)\n\n        # Compile model and set a small learning rate (we resume where the pretrained model stopped and we dont want to overshoot on the weights)\n        self.model.compile(run_eagerly=True, optimizer=Adam(\n            learning_rate=0.00001), loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])\n\n        callbacks = [\n            keras.callbacks.ModelCheckpoint(\n                \"best_model\", save_best_only=True, monitor='val_sparse_categorical_accuracy', save_format='tf',\n            ),\n        ]\n\n        # Fit model, should converge from 6 to 8 epochs. It will stop automatically when it reaches 100% accuracy on the training set.\n        self.model.fit(X_train, y, epochs=8,\n                       batch_size=32, validation_split=0.2, callbacks=callbacks)\n\n    def predict(self, X):\n        return self.model.predict(vgg16_preprocess_input(X), verbose=0)\n\n    def __call__(self, X):\n        return self.predict(X)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-12T18:08:02.034131Z","iopub.execute_input":"2023-04-12T18:08:02.034822Z","iopub.status.idle":"2023-04-12T18:08:02.046674Z","shell.execute_reply.started":"2023-04-12T18:08:02.034786Z","shell.execute_reply":"2023-04-12T18:08:02.045617Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"We will create an instance of the above network. We can print a summary that showcases the architecture behind the model (this output is a bit lengthy so we ignore it for now).","metadata":{}},{"cell_type":"code","source":"vgg_fe_cls = VGGFaceFeatureExtractorAndClassifier()\n## Remove comment to see architecture\n# vgg_fe_cls.model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-04-12T18:08:04.515599Z","iopub.execute_input":"2023-04-12T18:08:04.517799Z","iopub.status.idle":"2023-04-12T18:08:05.751031Z","shell.execute_reply.started":"2023-04-12T18:08:04.517750Z","shell.execute_reply":"2023-04-12T18:08:05.749986Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"To apply **transfer learning**, we have to finetune this model on our training set such that it predicts the right classes. This is computationally very expensive, but we can use a GPU in Kaggle to speed up the process.\n\n*Note:* we can load in the preprocessed training data here, in case one wants to immediately see the final performance.","metadata":{}},{"cell_type":"code","source":"## load preprocessed data\nprep_path = '/kaggle/working/prepped_data/'\nif os.path.exists(prep_path):\n    print(\"Loading preprocessed data\")\n    train_X = np.load(os.path.join(prep_path, 'train_X.npy'))\n    train_y = np.load(os.path.join(prep_path, 'train_y.npy'))","metadata":{"execution":{"iopub.execute_input":"2023-04-12T16:10:17.221200Z","iopub.status.busy":"2023-04-12T16:10:17.220915Z","iopub.status.idle":"2023-04-12T16:10:17.790274Z","shell.execute_reply":"2023-04-12T16:10:17.789119Z","shell.execute_reply.started":"2023-04-12T16:10:17.221172Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We fit (*i.e.*, fine-tune the final layers) based on our own training data. This heavily relies on the pretrained VGG architecture, but modifies the final layers for our personal purposes.","metadata":{}},{"cell_type":"code","source":"vgg_fe_cls.fit(train_X, train_y)","metadata":{"execution":{"iopub.execute_input":"2023-04-12T16:10:17.792479Z","iopub.status.busy":"2023-04-12T16:10:17.792061Z","iopub.status.idle":"2023-04-12T16:10:44.986214Z","shell.execute_reply":"2023-04-12T16:10:44.985169Z","shell.execute_reply.started":"2023-04-12T16:10:17.792431Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Experiments\n\nFace recognition is no easy task. Throughout designing this notebook, this team went through several stages in order to try and refine the complete pipeline. \n\nWe spent time on exploring different **preprocessors**, and also played around with several hyperparameters, such as an enlarged bounding box around the faces when extracting them from the images using our `cut_out_face` function. Besides this, we also explored various ways of editing the training data, of which our section on augmenting the dataset through manual crops is a reflection.\n\nFor the **SIFT** classifier, we tried using the training data obtained from different preprocessors, as well as varied the `FACE_SIZE` parameter. In the end, the data obtained from the HAAR detector together with the smaller face size (100, 100) seemed to give the best results for the sake of discussing the applications and drawbacks of SIFT. \n\nFor the **PCA**, we tried using both the eigenvalue decomposition as well as the singular value decomposition, and ultimately settled for the latter due to the advantage in terms of computational costs. We also explored various training sets (different preprocessors), and settled on the DeepFace data since the RMSD values seemed to be the best for this training set, although the MTCNN came in close at a second place. We also explored using color images as well as grayscale images, and decided to rely on the former, mainly for the sake of presentation of the notebook. Besides, we explored other measures of reconstruction loss, but settled on the RMSD measure as this seemed to be among the most commonly used ones and was easy to implement and run ourselves. \n\nFor the **classifiers**, the previous section clearly shows the different experiments that we tried: we mainly played around with trying different combinations of classification methods with different features. Besides this, there is a lot of experimentation still left for future work, such as tuning the hyperparameters of all the classifiers that we discussed above (think for instance about the endless possibilities in defining a feedforward MLP!). In the interest of time, we only scratched the surface of tuning the hyperparameters, and sticked to exploring different classifiers to enrich our know-how in this tutorial. \n\nIn the end, we did, however, briefly explore tuning the hyperparameters of the imported **VGG** architecture, as well as considered several submissions with different preprocessed data. As expected, the results with RetinaFace as preprocessor gave by far the best results.","metadata":{"papermill":{"duration":0.102942,"end_time":"2021-03-08T07:59:09.730342","exception":false,"start_time":"2021-03-08T07:59:09.6274","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## 4.0. Example: basic pipeline\n\nA demonstration of the basic and general pipeline we defined above is shown for the feedforward MLP. We do not delve deeper into showing different pipelines, since the *\"Classifiers\"* section already highlighted several combinations of feature extractors and classifiers. ","metadata":{}},{"cell_type":"markdown","source":"We use the `net` and `classifier` we defined above to create an instance of this classifier.","metadata":{}},{"cell_type":"code","source":"classifier = FeedForwardClassifier(net, scaler)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We now combine this MLP classifier with the PCA feature extractor to predict on the test set using our final pipeline.","metadata":{}},{"cell_type":"code","source":"predictions = prediction_pipeline(pca, classifier)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create CSV for submission","metadata":{}},{"cell_type":"code","source":"predictions_to_csv(predictions)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Publishing best results","metadata":{"papermill":{"duration":0.103853,"end_time":"2021-03-08T07:59:10.903341","exception":false,"start_time":"2021-03-08T07:59:10.799488","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"The best results were obtained with the VGG classifier. Since the architecture has its own preprocessing method, we do not rely on the `prediction_pipeline` function we defined above, but we slightly modify that `for` loop to work with the VGG classifier we imported and trained earlier on. To obtain the predictions on the test set, recall that we load in the preprocessed images we obtained with DeepFace, which is at the following file path:","metadata":{}},{"cell_type":"code","source":"print(f\"Loading preprocessed data from {test_prep_X_loc}\")\nprint(os.path.exists(test_prep_X_loc))  # must evaluate to True","metadata":{"execution":{"iopub.execute_input":"2023-04-12T16:07:11.176517Z","iopub.status.busy":"2023-04-12T16:07:11.175899Z","iopub.status.idle":"2023-04-12T16:07:11.185536Z","shell.execute_reply":"2023-04-12T16:07:11.184283Z","shell.execute_reply.started":"2023-04-12T16:07:11.176478Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Path points to a directory where preprocessed test data is located\ny_pred = []\n\n# Avoid errors here:\nif os.path.exists(test_prep_X_loc):\n    # Get the number of files\n    nb_of_files = len(os.listdir(test_prep_X_loc))\n    for i in tqdm(range(nb_of_files)):\n        # Load the image\n        img = np.load(os.path.join(test_prep_X_loc, f\"test_{i}.npy\"))\n        # Convert from BGR to RGB (warning! cv2.cvtColor gives error)\n        #img = img[...,::-1]\n        # VGG wants a specific shape as input, reshape\n        img = img.reshape((1,)+img.shape)\n        # Make the predictions: turn softmax into a prediction through argmax\n        softmax = vgg_fe_cls(img)\n        new_pred = np.argmax(softmax)\n        y_pred.append(new_pred)\n\n    os.path.exists(\"/kaggle/working\")\n    predictions_to_csv(y_pred, name=\"/kaggle/working/submission.csv\")","metadata":{"execution":{"iopub.execute_input":"2023-04-12T16:11:21.428872Z","iopub.status.busy":"2023-04-12T16:11:21.427825Z","iopub.status.idle":"2023-04-12T16:14:23.646267Z","shell.execute_reply":"2023-04-12T16:14:23.645290Z","shell.execute_reply.started":"2023-04-12T16:11:21.428817Z"},"pycharm":{"is_executing":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Discussion\n\nIn summary we contributed the following: \n* We enhanced and augmented the training dataset by manually cropping poorly cropped images or images that showed the wrong person.\n* We experimented with three preprocessing strategies, namely HAAR, MTCNN, and RetinaFace. Our final pipeline uses the RetinaFace preprocessing images because it combines face detection, landmark localization and face bounding box regression that result in the best accuracy in preprocessing faces.\n* We implemented SIFT to extract handcrafted features that are invariant to scale. We visualize these features and demonstrate how they are matched for facial recognition.\n* We implemented PCA to transform an image to a lower dimensional space that can be used as features.\n* We implemented various classification methods, namely SVM, RandomForest, and a FeedForwardNet on both SIFT and PCA features. We discover that the PCA features are more robust with a best average f1-score of .86. This may be due to the fact that the SIFT method extracts fewer good features, and thus may result in more variability during classification.\n* We also enhance our classifiers with cross validation and an ensemble method to classify data based on both SIFT and PCA features. These methods are especially important to tackle overfitting seen with such a small training dataset.\n* We used transfer learning to employ state of the art VGG Face Feature Extraction with results in 0.91 accuracy on the test data in the Kaggle competition.\n\nGiven more time we would focus on the following tasks:\n* The SIFT Feature Extractor was performing poorly on the test data. One potential reason for this is that SIFT only extracted a handful of 'good' features and demonstrated some matching features between lookalikes. In our experimentation, we noted that a smaller face size of (100,100) resulted in better extracted features compared to a face size of (224,224). Experimenting with an even smaller face size and tuning the SIFT parameters accordingly could lead to improved facial recognition based on SIFT.\n* The training dataset was limited to only 80 images. There exist deep learning models that create artificial variations of images to augment the dataset. Keras is one such library that contains an ImageDataGenerator class. This model could be investigated to expand the size of the training data. This would likely help against overfitting. Additionally rotated images could be added to the SIFT and VGG classifiers since they are invariant to rotation.\n* More time could be spent investigating state-of-the-art models such as VGG16 and DeepFace. These models required a lot of time and resources to tune. Better results could be achieved with more time or by investigating other deep learning architectures.\n* More time could be spent tuning hyperparameters. Particularly the number of principal components in our PCA Feature Extractor could be optimized for performance in our classifiers.\n* Incorporating prior knowledge can also help when training data is limited. Using the information from our confusion matrices for example could inform our classifiers of class imbalances and improve results.\n","metadata":{"papermill":{"duration":0.116655,"end_time":"2021-03-08T07:59:11.577703","exception":false,"start_time":"2021-03-08T07:59:11.461048","status":"completed"},"tags":[]}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}